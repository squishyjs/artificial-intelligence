{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEXooxuUo3gF"
   },
   "source": [
    "# Solving a Regression Problem using the K-Nearest Neighbours Algorithm\n",
    "Applying to a dataset (details below) to predict future 'HousePrice'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YuVBkPpko3gG"
   },
   "source": [
    "## Solving a regression problem\n",
    "In order to solve a regression problem, the following steps need to be taken:\n",
    "1. Find a suitable dataset\n",
    "2. Define the input dataset\n",
    "3. Explore the input dataset.\n",
    "After exploration, it may be needed to pre-process the dataset in real-world classification problems,i.e., to clean and pre-process the input dataset to make it ready for model development. In this example, the dataset is clean so there is no need for pre-processing.\n",
    "4. Form the training dataset and test dataset\n",
    "6. Train the regressor using the chosen algorithm\n",
    "7. Evaluate the regressor\n",
    "8. Deploy the regressor to solve the target regression problem (not covered in this notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCkT_El0o3gH"
   },
   "source": [
    "### Applying the KNN Model on a Dataset\n",
    "\n",
    "In order to solve the target regression problem, a HousePrice dataset was found from https://gist.github.com/grantbrown/5853625\n",
    "\n",
    "Information about this dataset: This dataset contains 429 items, where each item refers to a house. It has three columns as listed below:\n",
    "\n",
    "1. HouseAge in year\n",
    "2. HouseSize in $m^2$\n",
    "3. HousePrice in $\n",
    "\n",
    "The input features are `HouseAge` and `HouseSize` and the attribute to be predicted or target attribute/variable is `HousePrice`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jw1udNx8nlVS"
   },
   "source": [
    "## Initial Data Read and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WoKKKlBlnlVT"
   },
   "outputs": [],
   "source": [
    "# Import pandas package (library)\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4PvKXZ6Zo3gN"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('HousingData.csv') # call the Pandas method to read the data file: 'HousingData.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FeiUf42Go3gN"
   },
   "outputs": [],
   "source": [
    "# Take a look at the first few samples of data using the method head() of a pandas dataframe, by default, 5 samples\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TbiODs4fnlVU"
   },
   "source": [
    "## Section 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmFmVl4Lo3gP"
   },
   "source": [
    "### Form the training dataset, test dataset and futureSample set\n",
    "\n",
    "#### Create the training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NygjHVq4nlVV"
   },
   "outputs": [],
   "source": [
    "# Import the train_test_split from sklearn.model.selection\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npiEhNncnlVV"
   },
   "source": [
    "Define the input features X and the target attribute/variable using two ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rkrVyNvpnlVV"
   },
   "outputs": [],
   "source": [
    "# Solution -- Way 1\n",
    "\n",
    "# Define the input features X  directly\n",
    "X = data[['HouseAge', 'HouseSize']]\n",
    "y = data['HousePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "457cr1BHnlVV"
   },
   "outputs": [],
   "source": [
    "# Define the input features X as all columns except 'HousePrice' column and target variable y as the \"HousePrice\" column\n",
    "X = data.drop('HousePrice', axis=1)\n",
    "y = data['HousePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CsbYVQTIo3gQ"
   },
   "outputs": [],
   "source": [
    "# Using train_test_split function from sklearn to split the dataset into the training and test datasets, the percentage of samples in the test dataset is 20%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tsOn8TuDnlVV"
   },
   "outputs": [],
   "source": [
    "# Set aside futureSample test datasets\n",
    "# Take two samples from the testset as the future data samples, called futureSample_X, and futureSample_y,\n",
    "# as the inputs from the real-world cases when the classifier is deployed.\n",
    "\n",
    "# instantiated variables useful for predicting\n",
    "# Get the last two samples from the test  to be the future data samples\n",
    "futureSample_X = X_test[-2:]\n",
    "futureSample_y = y_test[-2:]\n",
    "\n",
    "# Remove the last two samples from the test dataset\n",
    "X_test = X_test[:-2]\n",
    "y_test = y_test[:-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1u8pZX2_nlVV"
   },
   "source": [
    "## Exploring the training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qc6viqMtnlVV"
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FMqwTGqKo3gQ"
   },
   "outputs": [],
   "source": [
    "# Example\n",
    "\n",
    "# Here, we visualise the distribution of the training data to understand the relationship betewen\n",
    "# house age and house price.\n",
    "plt.figure(1, figsize=(8, 6))\n",
    "plt.scatter(X_train['HouseAge'], y_train, c=y_train, cmap=plt.cm.Set1, edgecolor=\"k\")\n",
    "plt.xlabel(\"House Age\")\n",
    "plt.ylabel(\"House Price\")\n",
    "plt.title(\"Training data (age-price)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7StmJRF6nlVW"
   },
   "outputs": [],
   "source": [
    "# Using the example of plotting the samples in the training dataset (age-price), plot the samples in the training dataset (size-price)\n",
    "plt.figure(1, figsize=(8, 6))\n",
    "plt.scatter(X_train['HouseSize'], y_train, c=y_train, cmap=plt.cm.Set1, edgecolor=\"k\")\n",
    "plt.xlabel(\"House Size\")\n",
    "plt.ylabel(\"House Price\")\n",
    "plt.title(\"Training data (size-price)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X8fzEWE6nlVW"
   },
   "outputs": [],
   "source": [
    "# Example\n",
    "\n",
    "# plot the samples in the test dataset (age-price)\n",
    "plt.figure(3, figsize=(8, 6))\n",
    "plt.scatter(X_test['HouseAge'], y_test, c=y_test, cmap=plt.cm.Set1, edgecolor=\"k\")\n",
    "plt.xlabel(\"House Age\")\n",
    "plt.ylabel(\"House Price\")\n",
    "plt.title(\"Test data (age-price)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gu5-_RrunlVW"
   },
   "outputs": [],
   "source": [
    "# Use the example of plotting the samples in the test dataset (age-price), plot the samples in the test dataset (size-price)\n",
    "plt.figure(4, figsize=(8, 6))\n",
    "plt.scatter(X_test['HouseSize'], y_test, c=y_test, cmap=plt.cm.Set1, edgecolor=\"k\")\n",
    "plt.xlabel(\"House Size\")\n",
    "plt.ylabel(\"House Price\")\n",
    "plt.title(\"Test data (size-price)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WsAsgWOEnlVW"
   },
   "source": [
    "## Training the KNN Model using SKlearn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this Python Notebook, we focus on using the K-Nearest-Neighbour Algorithm to solve our Regression Problem: predicting 'HousePrice'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DFDTifaXo3gR"
   },
   "source": [
    "### Train a regressor by using a chosen learning algorithm with the trainig dataset.\n",
    "\n",
    "***First***, take an integer in the range of 1-5 from the user and save it to `model_option` to represent a model.\n",
    "\n",
    "`model_option`:\n",
    "* 1- Support vector regression,\n",
    "* 2- linear regression,\n",
    "* 3- K nearest neighbours and\n",
    "* 4- decision tree\n",
    "* 5- Random forest\n",
    "\n",
    "If a user enters a number  >5, print out a message \"invalid option number. Try again\".\n",
    "\n",
    "***Second***, train the model using the train dataset\n",
    "\n",
    "***Third***, evaluate the model using the test dataset\n",
    "\n",
    "***Lastly***, consume the model using the futureSample test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HN2mgMcfo3gR"
   },
   "outputs": [],
   "source": [
    "# creates a model (classifier learner model) selection prompt for user\n",
    "# NOTE: IN THIS PYTHON NOTEBOOKK, I DETAIL OPTION 3: K-NEAREST NEIGHBOURS\n",
    "model_option = int(input(\"Choose one model from the following: 1-Support vector regression, 2- linear regression, 3- K nearest neighbours, 4-decision tree and 5-Random Forest \\n your choice: \"))\n",
    "if (model_option == 1):\n",
    "    # Import SVR from sklearn.svm.\n",
    "    from sklearn.svm import SVR\n",
    "    \"\"\"\n",
    "    Call the constructor SVR() to create a SVR object, name it as 'model', by passing the following key parameters:\n",
    "\n",
    "    (i)  'kernel': Specifies the kernel type to be used in the algorithm. possible values are ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’ and ‘precomputed’.\n",
    "         Default is ’rbf’.\n",
    "    (ii) 'degree': Degree of the polynomial kernel function (‘poly’). Must be non-negative. Ignored by all other kernels. Default=3\n",
    "    (iii) 'gamma': Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’.\n",
    "          Possible values are ‘scale’ (1 / (n_features * X.var())) and ‘auto’ (1 / n_features).\n",
    "          Default is ’scale’.\n",
    "    (iv)  'C': Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive.\n",
    "        Default=1.0\n",
    "    (v) 'epsilon': Epsilon in the epsilon-SVR model. It specifies the epsilon-tube within which no penalty is associated in the training loss function with points predicted within a distance epsilon from the actual value.\n",
    "        Must be non-negative. Default=0.1\n",
    "    (vi) 'max_iter': Hard limit on iterations within solver, or -1 for no limit. Default is -1.\n",
    "    \"\"\"\n",
    "\n",
    "    model = SVR(gamma='auto')\n",
    "    # Train this model using the training dataset (X_train, y_train).\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "elif (model_option == 2):\n",
    "    #  Import LinearRegression from sklearn.linear_model.\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    \"\"\"\n",
    "    Call the constructor LinearRegression() to create a linear regression object, name it as 'model', by passing the following parameters:\n",
    "    (i) 'fit_intercep': Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (i.e. data is expected to be centered). Default=True\n",
    "    (ii) 'copy_X': If True, X will be copied; else, it may be overwritten. Default=True\n",
    "    \"\"\"\n",
    "    model = LinearRegression()\n",
    "    # Train this model using the training dataset (X_train, y_train).\n",
    "    model.fit(X_train, y_train)\n",
    "elif (model_option == 3):\n",
    "    # Import KNeighborsRegressor from sklearn.neighbors.\n",
    "    from sklearn.neighbors import KNeighborsRegressor\n",
    "    \n",
    "    # Here we are creating a KNN regressor object named 'model' using the KNieghboursRegressor class\n",
    "    # The key parameters passed to the constructor are:\n",
    "    # -n_neighbours=3: This specifies that the algorithm will consider 3 nearest neighbours\n",
    "    # when predicitng \n",
    "    # In other words, our \"k\" value for 'K-nearest-neighbours' is 3 -> this value is used for \n",
    "    # the prediction value of neighbouring nodes (data points)\n",
    "\n",
    "    \"\"\"\n",
    "    Call the constructor KNeighborsRegressor() to create a KNN regressor, name it as 'model', by passing the following parameters:\n",
    "    (i) 'n_neighbors': Number of neighbors to use by default for kneighbors queries. Default is 5.\n",
    "    (ii) 'weights': Weight function used in prediction. Possible values are ‘uniform’ (uniform weights),\n",
    "         ‘distance’ (weight points by the inverse of their distance) and\n",
    "         [callable] (a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights). Default = 'uniform'\n",
    "    (iii) 'algorithm': Algorithm used to compute the nearest neighbors. Possible values are ‘auto’ (attempt to decide the most appropriate algorithm based on the values passed to fit method),\n",
    "          ‘ball_tree’ (use BallTree), ‘kd_tree’ (use KDTree), and ‘brute’ (use a brute-force search). Default is ’auto’.\n",
    "    (iv) 'metric': Metric to use for distance computation. Default is “minkowski”, which results in the standard Euclidean distance when p = 2.\n",
    "    \"\"\"\n",
    "    model = KNeighborsRegressor(n_neighbors=3)\n",
    "    # Train this model using the training dataset (X_train, y_train).\n",
    "    # This line fits (trains) the KNN model on the features 'X_train' and the target y_train\n",
    "    # During this process, the model stores the training samples to use later for prediction\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # print statement indicating the KNN model is ready\n",
    "    print(\"KNN model is trained\")\n",
    "    \n",
    "elif (model_option == 4):\n",
    "    #  Import DecisionTreeRegressor from sklearn.tree.\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    \"\"\"\n",
    "    Call the constructor DecisionTreeRegressor () to create a decision tree regressor, name it as 'model', by passing the following key parameters:\n",
    "    (i) \"criterion\": The function to measure the quality of a split.\n",
    "         Possible options are “squared_error” (the mean squared error), “friedman_mse” ( mean squared error with Friedman’s improvement score),\n",
    "         “absolute_error” (the mean absolute error, which minimizes the L1 loss using the median of each terminal node), “poisson” (uses reduction in Poisson deviance to find splits).\n",
    "         Default=”squared_error”\n",
    "    (ii) \"splitter\": The strategy used to choose the split at each node.\n",
    "          Supported strategies are “best” to choose the best split and “random” to choose the best random split. Default is ”best”.\n",
    "    (iii) \"max_depth\": Maximum depth of the tree. If None, nodes are expanded until all leaves are pure or contains fewer than `min_samples_split` samples\n",
    "    (iv) \"min_samples_split\": Minimum number of samples required to split an internal node, default is 2\n",
    "    (v) \"min_samples_leaf\" Minimum number of samples required to be at a leaf node. default is 1.\n",
    "    (vi) \"max_features\": number of features to consider when looking for the best split, default is None (max_features=n_features).\n",
    "        Other values are 'sqrt' (max_features=sqrt(n_features)) and \"log2” (max_features=log2(n_features))\n",
    "    (vii) \"random_state\": Controls the randomness of the estimator. default is None.\n",
    "          To obtain a deterministic behaviour during fitting, random_state has to be fixed to an integer, say random_state = 42.\n",
    "    (viii) 'min_impurity_decrease': A node will be split if this split induces a decrease of the impurity greater than or equal to this value. Default=0.0.\n",
    "    \"\"\"\n",
    "    model = DecisionTreeRegressor()\n",
    "    # Train this DT regressor using the training data set (X_train, y_train).\n",
    "    model.fit(X_train, y_train)\n",
    "elif (model_option == 5):\n",
    "    # Import Random forest regreesor from sklearn.ensemble.\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    \"\"\"\n",
    "    Call the constructor RandomForestRegressor () to create a random forest regreesor, name it as 'model', by passing the following key parameters:\n",
    "    (i) 'n_estimator': Number of trees in the forest, default is 100\n",
    "    (ii) \"criterion\": The function to measure the quality of a split.\n",
    "         Possible options are “squared_error” (the mean squared error), “friedman_mse” ( mean squared error with Friedman’s improvement score),\n",
    "         “absolute_error” (the mean absolute error, which minimizes the L1 loss using the median of each terminal node), “poisson” (uses reduction in Poisson deviance to find splits).\n",
    "         Default=”squared_error”\n",
    "    (iii) \"max_depth\": Maximum depth of the tree. If None, nodes are expanded until all leaves are pure or contains fewer than `min_samples_split` samples\n",
    "    (iv) \"min_samples_split\": Minimum number of samples required to split an internal node, default is 2\n",
    "    (v) \"min_samples_leaf\" Minimum number of samples required to be at a leaf node. default is 1.\n",
    "    (vi) \"max_features\": number of features to consider when looking for the best split, default is None (max_features=n_features).\n",
    "        Other values are 'sqrt' (max_features=sqrt(n_features)) and \"log2” (max_features=log2(n_features)).\n",
    "    (vii) 'min_impurity_decrease': A node will be split if this split induces a decrease of the impurity greater than or equal to this value. Default=0.0.\n",
    "    (viii)'bootstrap': Whether to use bootstrap samples when building trees, default is 'True'.\n",
    "          If 'False', the entire dataset is used to build each tree, which may lead to overfitting.\n",
    "    (ix)   'oob_score': Whether to use out-of-bag samples to estimate the generalisation accuracy.\n",
    "          default value is 'False'. If 'True', an unbiased estimate of the model performance is provided.\n",
    "    (x) 'max_samples': If bootstrap is True, the number of samples to draw from X to train each base estimator. If None (default), then draw X.shape[0] samples. Default=None\n",
    "    (xi) \"random_state\": Controls both the randomness of the bootstrapping of the samples used when building trees (if bootstrap=True) and\n",
    "         the sampling of the features to consider when looking for the best split at each node (if max_features < n_features).\n",
    "         Default is None.\n",
    "    \"\"\"\n",
    "    model = RandomForestRegressor(n_estimators=3, max_depth=3, max_features=2, max_samples=100, random_state=12)\n",
    "    # Train this model using the training dataset (X_train, y_train).\n",
    "    model.fit(X_train, y_train)\n",
    "else: print(\"invalid option number. Try again\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTXEPI7qnlVW"
   },
   "source": [
    "## Section 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWID87B6nlVW"
   },
   "source": [
    "### Evaluate a regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXdDPMGio3gS"
   },
   "outputs": [],
   "source": [
    "# Import required package for evaluating a regression model\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hqiBWnVRo3gS"
   },
   "source": [
    "Test the regression model's performance using the method `predict()` to calculate the predicted values of test data and store the values in a variable, `solution_test`, and use a number of measures to evaluate the performance of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RrqUWlAXo3gS"
   },
   "outputs": [],
   "source": [
    "# Predicting the house prices for the test dataset (X_test)\n",
    "# The 'predict' method is used to make the predictions using the recently trained KNN model\n",
    "# It takes the features from X_test -> e.g., 'HouseAge' and 'HouseSize' and predicts the \n",
    "# corresponding house prices ('HousePrice' -> y vlaue)\n",
    "\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_2ZEtnAbnlVa"
   },
   "outputs": [],
   "source": [
    "# We now evaluate the model's performance, such that:\n",
    "# 1. We first calculate the R-squared score, which measure show well the predicted values match the\n",
    "# actual values\n",
    "# The higher the R-squared score (the closer it is to 1), the better the model's performance\n",
    "r2_test = metrics.r2_score(y_test, y_pred)\n",
    "print(r2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lbtWApNGnlVa"
   },
   "outputs": [],
   "source": [
    "# 2. We calculate the Mean Absolute Error (MAE), which measures the average magnitude of the errors\n",
    "# between the predicted and actual values\n",
    "# It's essentially a measure of how much the predictions DEVIATE from the actual values (on average),\n",
    "# allowing us to observe the overall accuracy of the KNN model's prediction (output value/data)\n",
    "mean_absolute_error_test = metrics.mean_absolute_error(y_test, y_pred) \n",
    "print(mean_absolute_error_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_aYIRT4WnlVa"
   },
   "outputs": [],
   "source": [
    "# We calculate the Mean Absolute Percentage Error (MAPE) -> this measures the prediction accuracy of \n",
    "# the KNN model, expressing such an accuracy as a percentage. A MAPE of 0% of indicates no error whereby\n",
    "# the model has a clear interpretation of the input/data value, and else otherwise.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define a function MAPE() which takes y as the true value and y_predict as the predicted value and returns the Mean Absolute Percentage Error over the test dateset. One sample's Absolute Percentage Error is calculated as: abs((y-y')*100/y)\n",
    "# Use the methods mean() and abs() in numpy\n",
    "def MAPE(y, y_predict):\n",
    "    return np.mean(np.abs((y - y_predict) / y)) * 100\n",
    "\n",
    "# Calculate the MAPE using the function MAPE() and y=y_test and y_predict = y_pred\n",
    "mape_test = MAPE(y_test, y_pred)\n",
    "print(mape_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9HhfCApznlVb"
   },
   "outputs": [],
   "source": [
    "# The RMSE (Root Mean Squared Error) is used to measure the differences between the values predicted\n",
    "# by the KNN model and the values actually observed. Similar to MAPE, we use the RMSE formula to \n",
    "# understand the accuracy of the model's prediction: lower values of RMSE indicate a 'better fit'\n",
    "\n",
    "# Calculate RMSE using the method sqrt() in numpy and mean_squared_error() in metrics\n",
    "rmse_test = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "print(rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y0Eo-CBRo3gT"
   },
   "outputs": [],
   "source": [
    "# We finally plot the relationship between the acutal house prices (y_test) and the predicted house \n",
    "# prices (y_pred)\n",
    "# The scatter plot is used to visually illustrate the differences in comparison to the actual values\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel('y_test') # label for ACTUAL house prices\n",
    "plt.ylabel('y_pred') # label for the PREDICTED house prices\n",
    "plt.show() # use the .show() method to visually display the scatter plot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8aI2SpJnlVb"
   },
   "source": [
    "## Section 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUDPfuKVo3gX"
   },
   "source": [
    "### Predict the values for the data samples in the future sample set\n",
    "Use the futureSample_X to simulate the new data items in a real-world application scenario. Use the method `predict()` to calculate the predicted values of futureSample data and store the values in a variable, `solution_validate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I9SJaxFBo3gY"
   },
   "outputs": [],
   "source": [
    "'''Essentially -> 'Generate the predicitions for future/test data we want to extrapolate' '''\n",
    "\n",
    "# Here, we essentially train the KNN model to make predictions on the futureSample_X, which \n",
    "# is the x_values of the dataset: namely, 'HouseAge' and 'HouseSize'\n",
    "# The .predict() method is called on the model, passing futureSample_X as the argument\n",
    "# Predicted values of samples in the futureSample dataset\n",
    "solution_validate = model.predict(futureSample_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jZD5g5pKo3gY"
   },
   "outputs": [],
   "source": [
    "# Convert the DataFrame to a list\n",
    "futureSample_X = futureSample_X.values.tolist() # necessary to ensure we iterate over the individual input features in a more simple, list-based format for the comparison later on\n",
    "futureSample_y = futureSample_y.tolist()\n",
    "# ^ this instantiated variable now contains the actual target values corresponding to each row in futureSample_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PGOdDu12nlVb"
   },
   "outputs": [],
   "source": [
    "'''Solution print the model's predictions and compare the predicted and actual values side by side,\n",
    "providing insight into how well the (KNN) model generalises the (unseen) data\n",
    "'''\n",
    "# Display the comparison of the predicted and actual values of samples in the futureSample dataset\n",
    "for i in range (2): # loop runs twice to compare the first two input data rows and their corresponding predictions\n",
    "    \n",
    "    print(\"For the {} future data, {}, the predicted value is {} and the actual value is {}\".format(i, futureSample_X[i], solution_validate[i], futureSample_y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''To create our own future/test data values, and utilise the KNN model to predict the y_value, HousePrice\n",
    "to extrapolate a corresponding prediction result'''\n",
    "\n",
    "house_age = float(input(\"Enter house age: \"))\n",
    "house_size = float(input(\"Enter house size: \"))\n",
    "\n",
    "x_values = [[house_age, house_size]]  # our custom datast of inputtted x_values -> HouseAge and HouseSize\n",
    "\n",
    "predicted_house_price = model.predict(x_values)\n",
    "\n",
    "print(f\"The preedicted house price for a house with age {house_age} and size {house_size} is: {predicted_house_price}.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "vp": {
   "vp_config_version": "1.0.0",
   "vp_menu_width": 273,
   "vp_note_display": false,
   "vp_note_width": 0,
   "vp_position": {
    "width": 278
   },
   "vp_section_display": false,
   "vp_signature": "VisualPython"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

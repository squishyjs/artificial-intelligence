{"cells":[{"cell_type":"markdown","metadata":{"id":"IEXooxuUo3gF"},"source":["## Lab 4 Solving a regression problem using techniques in Lec 4\n","Develop regressors using the techniques covered in Lec 4 to predict the value of a house price."]},{"cell_type":"markdown","metadata":{"id":"YuVBkPpko3gG"},"source":["## Solving a regression problem\n","In order to solve a regression problem, the following steps need to be taken:\n","1. Find a suitable dataset\n","2. Define the input dataset\n","3. Explore the input dataset.\n","After exploration, it may be needed to pre-process the dataset in real-world classification problems,i.e., to clean and pre-process the input dataset to make it ready for model development. In this example, the dataset is clean so there is no need for pre-processing.\n","4. Form the training dataset and test dataset\n","6. Train the regressor using the chosen algorithm\n","7. Evaluate the regressor\n","8. Deploy the regressor to solve the target regression problem (not covered in this notebook)"]},{"cell_type":"markdown","metadata":{"id":"UCkT_El0o3gH"},"source":["### Findnig a Dataset\n","\n","In order to solve the target regression problem, a HousePrice dataset is found from https://gist.github.com/grantbrown/5853625\n","\n","Information about this dataset: This dataset contains 429 items, where each item refers to a house. It has three columns as listed below:\n","\n","1. HouseAge in year\n","2. HouseSize in $m^2$\n","3. HousePrice in $\n","\n","The input features are `HouseAge` and `HouseSize` and the attribute to be predicted or target attribute/variable is `HousePrice`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3wJ8ALytqNGT"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZUWluNyeqmkc"},"outputs":[],"source":["cd /content/drive/MyDrive/IntroAILab/Lab4"]},{"cell_type":"markdown","metadata":{"id":"Jw1udNx8nlVS"},"source":["## Section 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"74W-8H_GnlVS"},"outputs":[],"source":["# Task\n","# Import pandas package\n","\n","\"Add your code here\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WoKKKlBlnlVT"},"outputs":[],"source":["# Solution\n","\n","# Import pandas package\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sFdXTc4zo3gM"},"outputs":[],"source":["# Task\n","# Load the dataset by reading the .csv file as a pandas dataframe\n","\n","\"Add your code here\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4PvKXZ6Zo3gN"},"outputs":[],"source":["# Solution\n","\n","data = pd.read_csv('HousingData.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"exX27tEdnlVU"},"outputs":[],"source":["# Task\n","\n","# Take a look at the first few samples of data using the method head() of a pandas dataframe, by default, 5 samples\n","\"Add your code here\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FeiUf42Go3gN"},"outputs":[],"source":["# Solution\n","\n","# Take a look at the first few samples of data using the method head() of a pandas dataframe, by default, 5 samples\n","print(data.head())"]},{"cell_type":"markdown","metadata":{"id":"TbiODs4fnlVU"},"source":["## Section 2"]},{"cell_type":"markdown","metadata":{"id":"hmFmVl4Lo3gP"},"source":["### Form the training dataset, test dataset and futureSample set\n","\n","#### Create the training and test datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FtCYtuemnlVU"},"outputs":[],"source":["# Task\n","\n","# Import train_test_split from sklearn.model.selection\n","\"Add your code here\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NygjHVq4nlVV"},"outputs":[],"source":["# Solution\n","\n","# Import train_test_split from sklearn.model.selection\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"markdown","metadata":{"id":"npiEhNncnlVV"},"source":["Define the input features X and the target attribute/variable using two ways"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FV-9S2PxnlVV"},"outputs":[],"source":["# Task -- Way 1\n","\n","# Define the input features X  directly\n","X = data[\"Add your code here\"]]\n","y = data[\"Add your code here\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rkrVyNvpnlVV"},"outputs":[],"source":["# Solution -- Way 1\n","\n","# Define the input features X  directly\n","X = data[['HouseAge', 'HouseSize']]\n","y = data['HousePrice']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zlhQbnYJnlVV"},"outputs":[],"source":["# Task -- Way 2\n","\n","# Define the input features X as all columns except 'HousePrice' column and target variable y as the \"HousePrice\" column\n","X = data.drop(\"Add your code here\", axis=1)\n","y = data[\"Add your code here\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"457cr1BHnlVV"},"outputs":[],"source":["# Solution -- Way 2\n","\n","# Define the input features X as all columns except 'HousePrice' column and target variable y as the \"HousePrice\" column\n","X = data.drop('HousePrice', axis=1)\n","y = data['HousePrice']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dq-uXV-Oo3gP"},"outputs":[],"source":["# Task\n","# Using train_test_split function from sklearn to split the dataset into the training and test datasets, the percentage of samples in the test dataset is 20%\n","\n","X_train, X_test, y_train, y_test = \"Add your code here\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CsbYVQTIo3gQ"},"outputs":[],"source":["# Solution\n","\n","# Using train_test_split function from sklearn to split the dataset into the training and test datasets, the percentage of samples in the test dataset is 20%\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PIOKmusnnlVV"},"outputs":[],"source":["# Task\n","\n","# Set aside futureSample test datasets\n","# Take two samples from the testset as the future data samples, called futureSample_X, and futureSample_y,\n","# as the inputs from the real-world cases when the classifier is deployed.\n","\n","# Get the last two samples from the test  to be the future data samples\n","futureSample_X = \"Add your code here\"\n","futureSample_y = \"Add your code here\"\n","\n","# Remove the last two samples from the test dataset\n","X_test = \"Add your code here\"\n","y_test = \"Add your code here\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tsOn8TuDnlVV"},"outputs":[],"source":["# Solution\n","\n","# Set aside futureSample test datasets\n","# Take two samples from the testset as the future data samples, called futureSample_X, and futureSample_y,\n","# as the inputs from the real-world cases when the classifier is deployed.\n","\n","# Get the last two samples from the test  to be the future data samples\n","futureSample_X = X_test[-2:]\n","futureSample_y = y_test[-2:]\n","\n","# Remove the last two samples from the test dataset\n","X_test = X_test[:-2]\n","y_test = y_test[:-2]"]},{"cell_type":"markdown","metadata":{"id":"1u8pZX2_nlVV"},"source":["## Section 3"]},{"cell_type":"markdown","metadata":{"id":"sS8DvfPZo3gQ"},"source":["### Explore the training and test datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x_LQ4SuAnlVV"},"outputs":[],"source":["# Task\n","\n","# import matplotlib.pyplot as plt\n","\"Add your code here\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qc6viqMtnlVV"},"outputs":[],"source":["# Solution\n","\n","# import matplotlib.pyplot as plt\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FMqwTGqKo3gQ"},"outputs":[],"source":["# Example\n","\n","# plot the samples in the training dataset (age-price)\n","plt.figure(1, figsize=(8, 6))\n","plt.scatter(X_train['HouseAge'], y_train, c=y_train, cmap=plt.cm.Set1, edgecolor=\"k\")\n","plt.xlabel(\"House Age\")\n","plt.ylabel(\"House Price\")\n","plt.title(\"Training data (age-price)\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7StmJRF6nlVW"},"outputs":[],"source":["# Solution\n","\n","# Using the example of plotting the samples in the training dataset (age-price), plot the samples in the training dataset (size-price)\n","plt.figure(1, figsize=(8, 6))\n","plt.scatter(X_train['HouseSize'], y_train, c=y_train, cmap=plt.cm.Set1, edgecolor=\"k\")\n","plt.xlabel(\"House Size\")\n","plt.ylabel(\"House Price\")\n","plt.title(\"Training data (size-price)\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X8fzEWE6nlVW"},"outputs":[],"source":["# Example\n","\n","# plot the samples in the test dataset (age-price)\n","plt.figure(3, figsize=(8, 6))\n","plt.scatter(X_test['HouseAge'], y_test, c=y_test, cmap=plt.cm.Set1, edgecolor=\"k\")\n","plt.xlabel(\"House Age\")\n","plt.ylabel(\"House Price\")\n","plt.title(\"Test data (age-price)\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gu5-_RrunlVW"},"outputs":[],"source":["# Solution\n","\n","# Use the example of plotting the samples in the test dataset (age-price), plot the samples in the test dataset (size-price)\n","plt.figure(4, figsize=(8, 6))\n","plt.scatter(X_test['HouseSize'], y_test, c=y_test, cmap=plt.cm.Set1, edgecolor=\"k\")\n","plt.xlabel(\"House Size\")\n","plt.ylabel(\"House Price\")\n","plt.title(\"Test data (size-price)\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"WsAsgWOEnlVW"},"source":["## Section 4"]},{"cell_type":"markdown","metadata":{"id":"DFDTifaXo3gR"},"source":["### Train a regressor by using a chosen learning algorithm with the trainig dataset.\n","\n","***First***, take an integer in the range of 1-5 from the user and save it to `model_option` to represent a model.\n","\n","`model_option`:\n","* 1- Support vector regression,\n","* 2- linear regression,\n","* 3- K nearest neighbours and\n","* 4- decision tree\n","* 5- Random forest\n","\n","If a user enters a number  >5, print out a message \"invalid option number. Try again\".\n","\n","***Second***, train the model using the train dataset\n","\n","***Third***, evaluate the model using the test dataset\n","\n","***Lastly***, consume the model using the futureSample test dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UB15HBKco3gR"},"outputs":[],"source":["# Tasks\n","model_option = \"Add your code here\"\n","if (model_option == 1):\n","    # Import SVR from sklearn.svm.\n","    \"Add your code here\"\n","     \"\"\"\n","    Call the constructor SVR() to create a SVR object, name it as 'model', by passing the following key parameters:\n","\n","    (i)  'kernel': Specifies the kernel type to be used in the algorithm. possible values are ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’ and ‘precomputed’.\n","         Default is ’rbf’.\n","    (ii) 'degree': Degree of the polynomial kernel function (‘poly’). Must be non-negative. Ignored by all other kernels. Default=3\n","    (iii) 'gamma': Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’.\n","          Possible values are ‘scale’ (1 / (n_features * X.var())) and ‘auto’ (1 / n_features).\n","          Default is ’scale’.\n","    (iv)  'C': Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive.\n","        Default=1.0\n","    (v) 'epsilon': Epsilon in the epsilon-SVR model. It specifies the epsilon-tube within which no penalty is associated in the training loss function with points predicted within a distance epsilon from the actual value.\n","        Must be non-negative. Default=0.1\n","    (vi) 'max_iter': Hard limit on iterations within solver, or -1 for no limit. Default is -1.\n","    \"\"\"\n","    \"Add your code here\"\n","    # Train this model using the training dataset (train_data, train_label).\n","    \"Add your code here\"\n","elif (model_option == 2):\n","    #  Import LinearRegression from sklearn.linear_model.\n","    \"Add your code here\"\n","    \"\"\"\n","    Call the constructor LinearRegression() to create a linear regression object, name it as 'model', by passing the following parameters:\n","    (i) 'fit_intercep': Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (i.e. data is expected to be centered). Default=True\n","    (ii) 'copy_X': If True, X will be copied; else, it may be overwritten. Default=True\n","    \"\"\"\n","    \"Add your code here\"\n","    # Train this model using the training dataset (train_data, train_label).\n","    \"Add your code here\"\n","elif (model_option == 3):\n","    # Import KNeighborsRegressor from sklearn.neighbors.\n","    \"Add your code here\"\n","    \"\"\"\n","    Call the constructor KNeighborsRegressor() to create a KNN regressor, name it as 'model', by passing the following parameters:\n","    (i) 'n_neighbors': Number of neighbors to use by default for kneighbors queries. Default is 5.\n","    (ii) 'weights': Weight function used in prediction. Possible values are ‘uniform’ (uniform weights),\n","         ‘distance’ (weight points by the inverse of their distance) and\n","         [callable] (a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights). Default = 'uniform'\n","    (iii) 'algorithm': Algorithm used to compute the nearest neighbors. Possible values are ‘auto’ (attempt to decide the most appropriate algorithm based on the values passed to fit method),\n","          ‘ball_tree’ (use BallTree), ‘kd_tree’ (use KDTree), and ‘brute’ (use a brute-force search). Default is ’auto’.\n","    (iv) 'metric': Metric to use for distance computation. Default is “minkowski”, which results in the standard Euclidean distance when p = 2.\n","    \"\"\"\n","    \"Add your code here\"\n","    # Train this model using the training dataset (train_data, train_label).\n","    \"Add your code here\"\n","elif (model_option == 4):\n","    #  Import DecisionTreeRegressor from sklearn.tree.\n","    \"Add your code here\"\n","    \"\"\"\n","    Call the constructor DecisionTreeRegressor () to create a decision tree regressor, name it as 'model', by passing the following key parameters:\n","    (i) \"criterion\": The function to measure the quality of a split.\n","         Possible options are “squared_error” (the mean squared error), “friedman_mse” ( mean squared error with Friedman’s improvement score),\n","         “absolute_error” (the mean absolute error, which minimizes the L1 loss using the median of each terminal node), “poisson” (uses reduction in Poisson deviance to find splits).\n","         Default=”squared_error”\n","    (ii) \"splitter\": The strategy used to choose the split at each node.\n","          Supported strategies are “best” to choose the best split and “random” to choose the best random split. Default is ”best”.\n","    (iii) \"max_depth\": Maximum depth of the tree. If None, nodes are expanded until all leaves are pure or contains fewer than `min_samples_split` samples\n","    (iv) \"min_samples_split\": Minimum number of samples required to split an internal node, default is 2\n","    (v) \"min_samples_leaf\" Minimum number of samples required to be at a leaf node. default is 1.\n","    (vi) \"max_features\": number of features to consider when looking for the best split, default is None (max_features=n_features).\n","        Other values are 'sqrt' (max_features=sqrt(n_features)) and \"log2” (max_features=log2(n_features))\n","    (vii) \"random_state\": Controls the randomness of the estimator. default is None.\n","          To obtain a deterministic behaviour during fitting, random_state has to be fixed to an integer, say random_state = 42.\n","    (viii) 'min_impurity_decrease': A node will be split if this split induces a decrease of the impurity greater than or equal to this value. Default=0.0.\n","    \"\"\"\n","    \"Add your code here\"\n","    # Train this DT regressor using the training data set (train_data, train_label).\n","    \"Add your code here\"\n","elif (model_option == 5):\n","    #  Import RandomForestRegressor from sklearn.ensenble.\n","    \"Add your code here\"\n","    \"\"\"\n","    Call the constructor RandomForestRegressor () to create a random forest regreesor, name it as 'model', by passing the following key parameters:\n","    (i) 'n_estimator': Number of trees in the forest, default is 100\n","    (ii) \"criterion\": The function to measure the quality of a split.\n","         Possible options are “squared_error” (the mean squared error), “friedman_mse” ( mean squared error with Friedman’s improvement score),\n","         “absolute_error” (the mean absolute error, which minimizes the L1 loss using the median of each terminal node), “poisson” (uses reduction in Poisson deviance to find splits).\n","         Default=”squared_error”\n","    (iii) \"max_depth\": Maximum depth of the tree. If None, nodes are expanded until all leaves are pure or contains fewer than `min_samples_split` samples\n","    (iv) \"min_samples_split\": Minimum number of samples required to split an internal node, default is 2\n","    (v) \"min_samples_leaf\" Minimum number of samples required to be at a leaf node. default is 1.\n","    (vi) \"max_features\": number of features to consider when looking for the best split, default is None (max_features=n_features).\n","        Other values are 'sqrt' (max_features=sqrt(n_features)) and \"log2” (max_features=log2(n_features)).\n","    (vii) 'min_impurity_decrease': A node will be split if this split induces a decrease of the impurity greater than or equal to this value. Default=0.0.\n","    (viii)'bootstrap': Whether to use bootstrap samples when building trees, default is 'True'.\n","          If 'False', the entire dataset is used to build each tree, which may lead to overfitting.\n","    (ix)   'oob_score': Whether to use out-of-bag samples to estimate the generalisation accuracy.\n","          default value is 'False'. If 'True', an unbiased estimate of the model performance is provided.\n","    (x) 'max_samples': If bootstrap is True, the number of samples to draw from X to train each base estimator. If None (default), then draw X.shape[0] samples. Default=None\n","    (xi) \"random_state\": Controls both the randomness of the bootstrapping of the samples used when building trees (if bootstrap=True) and\n","         the sampling of the features to consider when looking for the best split at each node (if max_features < n_features).\n","         Default is None.\n","    \"\"\"\n","    \"Add your code here\"\n","    # Train this random forest regressor using the training data set (train_data, train_label).\n","    \"Add your code here\"\n","else: print(\"invalid option number. Try again\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HN2mgMcfo3gR"},"outputs":[],"source":["# Solution\n","model_option = int(input(\"Choose one model from the following: 1-Support vector regression, 2- linear regression, 3- K nearest neighbours, 4-decision tree and 5-Random Forest \\n your choice: \"))\n","if (model_option == 1):\n","    # Import SVR from sklearn.svm.\n","    from sklearn.svm import SVR\n","    \"\"\"\n","    Call the constructor SVR() to create a SVR object, name it as 'model', by passing the following key parameters:\n","\n","    (i)  'kernel': Specifies the kernel type to be used in the algorithm. possible values are ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’ and ‘precomputed’.\n","         Default is ’rbf’.\n","    (ii) 'degree': Degree of the polynomial kernel function (‘poly’). Must be non-negative. Ignored by all other kernels. Default=3\n","    (iii) 'gamma': Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’.\n","          Possible values are ‘scale’ (1 / (n_features * X.var())) and ‘auto’ (1 / n_features).\n","          Default is ’scale’.\n","    (iv)  'C': Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive.\n","        Default=1.0\n","    (v) 'epsilon': Epsilon in the epsilon-SVR model. It specifies the epsilon-tube within which no penalty is associated in the training loss function with points predicted within a distance epsilon from the actual value.\n","        Must be non-negative. Default=0.1\n","    (vi) 'max_iter': Hard limit on iterations within solver, or -1 for no limit. Default is -1.\n","    \"\"\"\n","    model = SVR(gamma='auto')\n","    # Train this model using the training dataset (X_train, y_train).\n","    model.fit(X_train, y_train)\n","elif (model_option == 2):\n","    #  Import LinearRegression from sklearn.linear_model.\n","    from sklearn.linear_model import LinearRegression\n","    \"\"\"\n","    Call the constructor LinearRegression() to create a linear regression object, name it as 'model', by passing the following parameters:\n","    (i) 'fit_intercep': Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (i.e. data is expected to be centered). Default=True\n","    (ii) 'copy_X': If True, X will be copied; else, it may be overwritten. Default=True\n","    \"\"\"\n","    model = LinearRegression()\n","    # Train this model using the training dataset (X_train, y_train).\n","    model.fit(X_train, y_train)\n","elif (model_option == 3):\n","    # Import KNeighborsRegressor from sklearn.neighbors.\n","    from sklearn.neighbors import KNeighborsRegressor\n","    \"\"\"\n","    Call the constructor KNeighborsRegressor() to create a KNN regressor, name it as 'model', by passing the following parameters:\n","    (i) 'n_neighbors': Number of neighbors to use by default for kneighbors queries. Default is 5.\n","    (ii) 'weights': Weight function used in prediction. Possible values are ‘uniform’ (uniform weights),\n","         ‘distance’ (weight points by the inverse of their distance) and\n","         [callable] (a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights). Default = 'uniform'\n","    (iii) 'algorithm': Algorithm used to compute the nearest neighbors. Possible values are ‘auto’ (attempt to decide the most appropriate algorithm based on the values passed to fit method),\n","          ‘ball_tree’ (use BallTree), ‘kd_tree’ (use KDTree), and ‘brute’ (use a brute-force search). Default is ’auto’.\n","    (iv) 'metric': Metric to use for distance computation. Default is “minkowski”, which results in the standard Euclidean distance when p = 2.\n","    \"\"\"\n","    model = KNeighborsRegressor(n_neighbors=3)\n","    # Train this model using the training dataset (X_train, y_train).\n","    model.fit(X_train, y_train)\n","elif (model_option == 4):\n","    #  Import DecisionTreeRegressor from sklearn.tree.\n","    from sklearn.tree import DecisionTreeRegressor\n","    \"\"\"\n","    Call the constructor DecisionTreeRegressor () to create a decision tree regressor, name it as 'model', by passing the following key parameters:\n","    (i) \"criterion\": The function to measure the quality of a split.\n","         Possible options are “squared_error” (the mean squared error), “friedman_mse” ( mean squared error with Friedman’s improvement score),\n","         “absolute_error” (the mean absolute error, which minimizes the L1 loss using the median of each terminal node), “poisson” (uses reduction in Poisson deviance to find splits).\n","         Default=”squared_error”\n","    (ii) \"splitter\": The strategy used to choose the split at each node.\n","          Supported strategies are “best” to choose the best split and “random” to choose the best random split. Default is ”best”.\n","    (iii) \"max_depth\": Maximum depth of the tree. If None, nodes are expanded until all leaves are pure or contains fewer than `min_samples_split` samples\n","    (iv) \"min_samples_split\": Minimum number of samples required to split an internal node, default is 2\n","    (v) \"min_samples_leaf\" Minimum number of samples required to be at a leaf node. default is 1.\n","    (vi) \"max_features\": number of features to consider when looking for the best split, default is None (max_features=n_features).\n","        Other values are 'sqrt' (max_features=sqrt(n_features)) and \"log2” (max_features=log2(n_features))\n","    (vii) \"random_state\": Controls the randomness of the estimator. default is None.\n","          To obtain a deterministic behaviour during fitting, random_state has to be fixed to an integer, say random_state = 42.\n","    (viii) 'min_impurity_decrease': A node will be split if this split induces a decrease of the impurity greater than or equal to this value. Default=0.0.\n","    \"\"\"\n","    model = DecisionTreeRegressor()\n","    # Train this DT regressor using the training data set (X_train, y_train).\n","    model.fit(X_train, y_train)\n","elif (model_option == 5):\n","    # Import Random forest regreesor from sklearn.ensemble.\n","    from sklearn.ensemble import RandomForestRegressor\n","    \"\"\"\n","    Call the constructor RandomForestRegressor () to create a random forest regreesor, name it as 'model', by passing the following key parameters:\n","    (i) 'n_estimator': Number of trees in the forest, default is 100\n","    (ii) \"criterion\": The function to measure the quality of a split.\n","         Possible options are “squared_error” (the mean squared error), “friedman_mse” ( mean squared error with Friedman’s improvement score),\n","         “absolute_error” (the mean absolute error, which minimizes the L1 loss using the median of each terminal node), “poisson” (uses reduction in Poisson deviance to find splits).\n","         Default=”squared_error”\n","    (iii) \"max_depth\": Maximum depth of the tree. If None, nodes are expanded until all leaves are pure or contains fewer than `min_samples_split` samples\n","    (iv) \"min_samples_split\": Minimum number of samples required to split an internal node, default is 2\n","    (v) \"min_samples_leaf\" Minimum number of samples required to be at a leaf node. default is 1.\n","    (vi) \"max_features\": number of features to consider when looking for the best split, default is None (max_features=n_features).\n","        Other values are 'sqrt' (max_features=sqrt(n_features)) and \"log2” (max_features=log2(n_features)).\n","    (vii) 'min_impurity_decrease': A node will be split if this split induces a decrease of the impurity greater than or equal to this value. Default=0.0.\n","    (viii)'bootstrap': Whether to use bootstrap samples when building trees, default is 'True'.\n","          If 'False', the entire dataset is used to build each tree, which may lead to overfitting.\n","    (ix)   'oob_score': Whether to use out-of-bag samples to estimate the generalisation accuracy.\n","          default value is 'False'. If 'True', an unbiased estimate of the model performance is provided.\n","    (x) 'max_samples': If bootstrap is True, the number of samples to draw from X to train each base estimator. If None (default), then draw X.shape[0] samples. Default=None\n","    (xi) \"random_state\": Controls both the randomness of the bootstrapping of the samples used when building trees (if bootstrap=True) and\n","         the sampling of the features to consider when looking for the best split at each node (if max_features < n_features).\n","         Default is None.\n","    \"\"\"\n","    model = RandomForestRegressor(n_estimators=3, max_depth=3, max_features=2, max_samples=100, random_state=12)\n","    # Train this model using the training dataset (X_train, y_train).\n","    model.fit(X_train, y_train)\n","else: print(\"invalid option number. Try again\")\n"]},{"cell_type":"markdown","metadata":{"id":"gTXEPI7qnlVW"},"source":["## Section 5"]},{"cell_type":"markdown","metadata":{"id":"sWID87B6nlVW"},"source":["### Evaluate a regression model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vXdDPMGio3gS"},"outputs":[],"source":["# Import required package for evaluating a regression model\n","from sklearn import metrics\n","from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix"]},{"cell_type":"markdown","metadata":{"id":"hqiBWnVRo3gS"},"source":["Test the regression model's performance using the method `predict()` to calculate the predicted values of test data and store the values in a variable, `solution_test`, and use a number of measures to evaluate the performance of this model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VMS_psuFo3gS"},"outputs":[],"source":["# Task\n","\n","# Predicted values of samples in the test dataset\n","y_pred = \"Add your code here\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RrqUWlAXo3gS"},"outputs":[],"source":["# Solution\n","\n","# Predicted values of samples in the test dataset\n","y_pred = model.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9sXddxTqo3gS"},"outputs":[],"source":["# Task\n","\n","# Calculate the R square of predicted values of samples in the test dataset using the method r1_score() in metrics\n","r2_test = \"Add your code here\"\n","print(r1_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_2ZEtnAbnlVa"},"outputs":[],"source":["# Solution\n","\n","# Calculate the R square of predicted values of samples in the test dataset using the method r1_score() in metrics\n","r2_test = metrics.r2_score(y_test, y_pred)\n","print(r2_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LkmOl2jfo3gS"},"outputs":[],"source":["# Task\n","\n","# Calculate the MAE(Mean Absolute Error) of predicted values of samples in the test dataset using the method mean_absolute_error() in metrics\n","mean_absolute_error_test = \"Add your code here\"\n","print(mean_absolute_error_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lbtWApNGnlVa"},"outputs":[],"source":["# Solution\n","\n","# Calculate the MAE(Mean Absolute Error) of predicted values of samples in the test dataset using the method mean_absolute_error() in metrics\n","mean_absolute_error_test = metrics.mean_absolute_error(y_test, y_pred)\n","print(mean_absolute_error_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vn8f7OV0o3gT"},"outputs":[],"source":["# Tasks\n","\n","# Import numpy as np\n","\"Add your code here\"\n","\n","# Define a function MAPE() which takes y as the true value and y_predict as the predicted value and returns the Mean Absolute Percentage Error over the test dateset. One sample's Absolute Percentage Error is calculated as: abs((y-y')*100/y)\n","# Use the methods mean() and abs() in numpy\n","def MAPE(\"Add your code here\"):\n","    return \"Add your code here\"\n","\n","# Calculate the MAPE using the function MAPE() and y=y_test and y_predict = y_pred\n","\"Add your code here\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_aYIRT4WnlVa"},"outputs":[],"source":["# Solution\n","\n","# Import numpy as np\n","import numpy as np\n","\n","# Define a function MAPE() which takes y as the true value and y_predict as the predicted value and returns the Mean Absolute Percentage Error over the test dateset. One sample's Absolute Percentage Error is calculated as: abs((y-y')*100/y)\n","# Use the methods mean() and abs() in numpy\n","def MAPE(y, y_predict):\n","    return np.mean(np.abs((y - y_predict) / y)) * 100\n","\n","# Calculate the MAPE using the function MAPE() and y=y_test and y_predict = y_pred\n","mape_test = MAPE(y_test, y_pred)\n","print(mape_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pgdp8CAFo3gT"},"outputs":[],"source":["# Task\n","\n","# Calculate RMSE(Root Mean Squared Error) using the method sqrt() in numpy and mean_squared_error() in metrics\n","rmse_test =\"Add your code here\"\n","print(rmse_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9HhfCApznlVb"},"outputs":[],"source":["# Solution\n","\n","# Calculate RMSE(Root Mean Squared Error) using the method sqrt() in numpy and mean_squared_error() in metrics\n","rmse_test = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n","print(rmse_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y0Eo-CBRo3gT"},"outputs":[],"source":["# Present the regression plot\n","plt.scatter(y_test, y_pred)\n","plt.xlabel('y_test')\n","plt.ylabel('y_pred')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"P8aI2SpJnlVb"},"source":["## Section 6"]},{"cell_type":"markdown","metadata":{"id":"jUDPfuKVo3gX"},"source":["### Predict the values for the data samples in the future sample set\n","Use the futureSample_X to simulate the new data items in a real-world application scenario. Use the method `predict()` to calculate the predicted values of futureSample data and store the values in a variable, `solution_validate`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"szgMWYB6o3gY"},"outputs":[],"source":["# Task\n","\n","# Predicted values of samples in the futureSample dataset\n","solution_validate = \"Add your code here\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I9SJaxFBo3gY"},"outputs":[],"source":["# Solution\n","\n","# Predicted values of samples in the futureSample dataset\n","solution_validate = model.predict(futureSample_X)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jZD5g5pKo3gY"},"outputs":[],"source":["# Convert the DataFrame to a list\n","futureSample_X = futureSample_X.values.tolist()\n","futureSample_y = futureSample_y.tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fwmxDmP8nlVb"},"outputs":[],"source":["# Task\n","\n","# Display the comparison of the predicted and actual values of samples in the futureSample dataset\n","for i in range (2):\n","    print(\"For the {} future data, the predicted value is {} and the actual value is {}\".format(\"Add your code here\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PGOdDu12nlVb"},"outputs":[],"source":["# Solution\n","\n","# Display the comparison of the predicted and actual values of samples in the futureSample dataset\n","for i in range (2):\n","    print(\"For the {} future data, {}, the predicted value is {} and the actual value is {}\".format(i, futureSample_X[i], solution_validate[i], futureSample_y[i]))"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"pycharm":{"stem_cell":{"cell_type":"raw","metadata":{"collapsed":false},"source":[]}},"vp":{"vp_config_version":"1.0.0","vp_menu_width":273,"vp_note_display":false,"vp_note_width":0,"vp_position":{"width":278},"vp_section_display":false,"vp_signature":"VisualPython"}},"nbformat":4,"nbformat_minor":0}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xvXwpe-Wvog"
   },
   "source": [
    "## Lab 8 NLP basics\n",
    "\n",
    "In this notebook, you will learn how the basic NLP methods work. The methods include:\n",
    "* Extract words from a given text using word-tokenisation\n",
    "* Create n-grams from files in a given directory,\n",
    "* Generate word meaning vector using a Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24419,
     "status": "ok",
     "timestamp": 1696396831989,
     "user": {
      "displayName": "Hai Yan (Helen) Lu",
      "userId": "06999938304037686402"
     },
     "user_tz": -660
    },
    "id": "6P1vMuE2W4qM",
    "outputId": "648820aa-bc68-451b-c952-65570658f6c6"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1696396855305,
     "user": {
      "displayName": "Hai Yan (Helen) Lu",
      "userId": "06999938304037686402"
     },
     "user_tz": -660
    },
    "id": "E4kPac6FW40q",
    "outputId": "e3d21642-b7d9-4ad0-fe03-32676f335fb7"
   },
   "outputs": [],
   "source": [
    "cd /content/drive/MyDrive/Lab2024/Lab8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5026,
     "status": "ok",
     "timestamp": 1696396864859,
     "user": {
      "displayName": "Hai Yan (Helen) Lu",
      "userId": "06999938304037686402"
     },
     "user_tz": -660
    },
    "id": "m95Qb3ZDWvoi",
    "outputId": "918d0643-c009-4bfa-a691-42fa2ec6d50d"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"popular\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 941,
     "status": "ok",
     "timestamp": 1696396897666,
     "user": {
      "displayName": "Hai Yan (Helen) Lu",
      "userId": "06999938304037686402"
     },
     "user_tz": -660
    },
    "id": "Lq-muOTiWvok"
   },
   "outputs": [],
   "source": [
    "from heading import *\n",
    "from nltk.tokenize import *\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-R70xS7Wvol"
   },
   "source": [
    "## Tokenisation\n",
    "Tokenisation is to split a text into meaningful units. Sentence tokenization will produce tokens as sentences and Word tokenization is to produce tokens as words.\n",
    "\n",
    "Here you will try an example of Sentence tokenization and one example of Word tokenization.\n",
    "\n",
    "First you need to define a text that needs to be tokenised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 270,
     "status": "ok",
     "timestamp": 1696396905094,
     "user": {
      "displayName": "Hai Yan (Helen) Lu",
      "userId": "06999938304037686402"
     },
     "user_tz": -660
    },
    "id": "7rOoukZEWvol"
   },
   "outputs": [],
   "source": [
    "EXAMPLE_TEXT = \"Hello Mr. Smith, how are you doing today? The weather is great, and Python is awesome. The sky is pinkish-blue. You shouldn't eat cardboard.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIH65f_sWvom"
   },
   "source": [
    "Then you use the method `sent_tokenize()` to separate the text into sentences and print out the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 280,
     "status": "ok",
     "timestamp": 1696396908659,
     "user": {
      "displayName": "Hai Yan (Helen) Lu",
      "userId": "06999938304037686402"
     },
     "user_tz": -660
    },
    "id": "Bt_YKFJOWvon",
    "outputId": "df3c85fb-5248-434a-e4a2-063dd6dab186"
   },
   "outputs": [],
   "source": [
    "sent_tokenize_result = sent_tokenize(EXAMPLE_TEXT)\n",
    "print(sent_tokenize_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WiIzbAQqWvoo"
   },
   "source": [
    "You can also use the method `word_tokenize()` to separate the text into words and print out the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 402,
     "status": "ok",
     "timestamp": 1696396912120,
     "user": {
      "displayName": "Hai Yan (Helen) Lu",
      "userId": "06999938304037686402"
     },
     "user_tz": -660
    },
    "id": "3RxepZ0pWvoo",
    "outputId": "13f5c466-4f33-4195-92a6-f91f03e824bd"
   },
   "outputs": [],
   "source": [
    "word_tokenize_result = word_tokenize(EXAMPLE_TEXT)\n",
    "print(word_tokenize_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 531,
     "status": "ok",
     "timestamp": 1696396925980,
     "user": {
      "displayName": "Hai Yan (Helen) Lu",
      "userId": "06999938304037686402"
     },
     "user_tz": -660
    },
    "id": "hc0wm9iiWvop",
    "outputId": "91452719-fe4f-42fe-d184-b705914efa83"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "all_punctuation = string.punctuation\n",
    "print(f\"All punctuation: {all_punctuation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 294,
     "status": "ok",
     "timestamp": 1696396930143,
     "user": {
      "displayName": "Hai Yan (Helen) Lu",
      "userId": "06999938304037686402"
     },
     "user_tz": -660
    },
    "id": "07JuOjK4Wvoq"
   },
   "outputs": [],
   "source": [
    "# You can configure for the language you need. In this example, you can use 'English'\n",
    "stops = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1696396933145,
     "user": {
      "displayName": "Hai Yan (Helen) Lu",
      "userId": "06999938304037686402"
     },
     "user_tz": -660
    },
    "id": "So0AA9p0Wvoq",
    "outputId": "775e9a62-0b5c-40f0-dc13-89cace58d237"
   },
   "outputs": [],
   "source": [
    "#Filter the stop words and punctuations etc by removing the stop words and punctuations in the word list\n",
    "removales = list(stops)+list(all_punctuation)+list(\"n't\")\n",
    "wordsWithoutStopWords = []\n",
    "\n",
    "for w in word_tokenize_result:\n",
    "    w_lower = w.lower()\n",
    "    if (w_lower not in stops) and (w_lower!=\"n't\") and (w_lower not in all_punctuation):\n",
    "        wordsWithoutStopWords.append(w)\n",
    "\n",
    "print(wordsWithoutStopWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3b4_KZ0vWvor"
   },
   "source": [
    "### N-grams\n",
    "\n",
    "A sequence of written items of length N is called an N-gram. The unigram (or 1-gram), bigram (or 2-gram), and trigram (or 3-gram) are sequences of one, two, and three items, respectively.\n",
    "\n",
    "In a character N-gram, the items are characters, and in a word N-gram the items are words. Here we practice to generate word N-grams for a given sentence in a simple application example.\n",
    "\n",
    "Example: Extract the number of occurrences of N-grams in a given document.\n",
    "\n",
    "First you need to specify two pieces of information:\n",
    "* the path to a folder containing the given document files and\n",
    "* the N in N-grams, which is the number of consecutive words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 270,
     "status": "ok",
     "timestamp": 1696396944479,
     "user": {
      "displayName": "Hai Yan (Helen) Lu",
      "userId": "06999938304037686402"
     },
     "user_tz": -660
    },
    "id": "rH67HDZEWvor"
   },
   "outputs": [],
   "source": [
    "file_directory = \"text_data/holmes\"\n",
    "num_words = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_N55P3zWvor"
   },
   "source": [
    "Then, you need to create an instance of `Ngrams` using the directory path and the N value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1696396947063,
     "user": {
      "displayName": "Hai Yan (Helen) Lu",
      "userId": "06999938304037686402"
     },
     "user_tz": -660
    },
    "id": "69wvwDhLWvor"
   },
   "outputs": [],
   "source": [
    "ngrams = Ngrams(file_directory,num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yd9HA2JOWvos"
   },
   "source": [
    "Now you can get frequencies of N-grams in the given corpus of documents. Here we display top 10 N-grams with their frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7614,
     "status": "ok",
     "timestamp": 1696396957243,
     "user": {
      "displayName": "Hai Yan (Helen) Lu",
      "userId": "06999938304037686402"
     },
     "user_tz": -660
    },
    "id": "_PFAUGoJWvos",
    "outputId": "e0a7f5af-d34b-4204-8caf-1395200ee993"
   },
   "outputs": [],
   "source": [
    "ngrams.top_term_frequencies(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iv9eE9a_Wvos"
   },
   "source": [
    "You can generate the n-grams from a given corpus, here is an example of getting 2-grams of a test string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 285,
     "status": "ok",
     "timestamp": 1696396962168,
     "user": {
      "displayName": "Hai Yan (Helen) Lu",
      "userId": "06999938304037686402"
     },
     "user_tz": -660
    },
    "id": "BKVRY3UhWvos",
    "outputId": "4f4819d1-bfa9-43b1-97a7-8a3aa0cd07b1"
   },
   "outputs": [],
   "source": [
    "ngrams_test = Counter(nltk.ngrams(wordsWithoutStopWords,num_words))\n",
    "print(ngrams_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 280,
     "status": "ok",
     "timestamp": 1696396976068,
     "user": {
      "displayName": "Hai Yan (Helen) Lu",
      "userId": "06999938304037686402"
     },
     "user_tz": -660
    },
    "id": "deDqfktUWvot",
    "outputId": "ec3a1146-445b-46bd-a296-5f82961de899"
   },
   "outputs": [],
   "source": [
    "for ngram, freq in ngrams_test.most_common(5):\n",
    "    print(f\"{freq}: {ngram}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1n83TqEWWvot"
   },
   "source": [
    "### Generate a word vector to represent the meaning of a word using a Word2Vec model\n",
    "\n",
    "Use a Word2vec model you can generate a word vector to represent the word meaning.\n",
    "\n",
    "First, you need to define the path of a text file `words.txt` representing the word2vec model.\n",
    "In the following example, the file is in a local folder `text_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 292,
     "status": "ok",
     "timestamp": 1696396981275,
     "user": {
      "displayName": "Hai Yan (Helen) Lu",
      "userId": "06999938304037686402"
     },
     "user_tz": -660
    },
    "id": "Cc4hZMwuWvot"
   },
   "outputs": [],
   "source": [
    "file_directory = \"text_data/words.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twMddEmwWvou"
   },
   "source": [
    "Then, you can create an instance of `Vectors` class by calling the method `Vectors(localtion-your-word2vec-model-file)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2786,
     "status": "ok",
     "timestamp": 1696396986547,
     "user": {
      "displayName": "Hai Yan (Helen) Lu",
      "userId": "06999938304037686402"
     },
     "user_tz": -660
    },
    "id": "M8muGmh3Wvou"
   },
   "outputs": [],
   "source": [
    "vectors = Vectors(file_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPvG9xpFWvou"
   },
   "source": [
    "Lastly, you can get the word model by calling the method `words` through the instance of `Vectors` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 277,
     "status": "ok",
     "timestamp": 1696397004292,
     "user": {
      "displayName": "Hai Yan (Helen) Lu",
      "userId": "06999938304037686402"
     },
     "user_tz": -660
    },
    "id": "Iig2_SKnWvou"
   },
   "outputs": [],
   "source": [
    "words = vectors.words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jlZaObdWvou"
   },
   "source": [
    "The elements in a word vector don't mean much. However, you can use the word vector to do the following things:\n",
    "* find out the distance between two words,\n",
    "* find out the closet of a word, and\n",
    "* find out the relationship between related words\n",
    "\n",
    "Now you can see some examples of word vectors and how to use them.\n",
    "\n",
    "Here are some examples:\n",
    "* word vector of \"city\"\n",
    "* disctance between \"city\" and \"book\"\n",
    "* top 10 closest words to word \"book\"\n",
    "* calculate a new word vector from \"paris\" - \"france\" + \"england\" and find the closest word to this new word\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 390,
     "status": "ok",
     "timestamp": 1696397007908,
     "user": {
      "displayName": "Hai Yan (Helen) Lu",
      "userId": "06999938304037686402"
     },
     "user_tz": -660
    },
    "id": "EWiCYPQ_Wvou",
    "outputId": "39c7306c-0686-4a89-810c-6dce5b61922c"
   },
   "outputs": [],
   "source": [
    "words[\"city\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7_i5qYHWvov"
   },
   "source": [
    "You can calculate the vectors' distance between two different words by `distance()` method in `Vectors()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 298,
     "status": "ok",
     "timestamp": 1696397011301,
     "user": {
      "displayName": "Hai Yan (Helen) Lu",
      "userId": "06999938304037686402"
     },
     "user_tz": -660
    },
    "id": "jOBueniNWvov",
    "outputId": "fb4afbf7-b6b7-4117-c83f-cd5d2e9e82f5"
   },
   "outputs": [],
   "source": [
    "vectors.distance(words[\"city\"], words[\"book\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PhjL0yyXWvov"
   },
   "source": [
    "You can also get the first 10 closest words by `closest_words()` method in `Vectors()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1667,
     "status": "ok",
     "timestamp": 1696397016161,
     "user": {
      "displayName": "Hai Yan (Helen) Lu",
      "userId": "06999938304037686402"
     },
     "user_tz": -660
    },
    "id": "VedrEuSgWvov",
    "outputId": "a2ff58d0-3f59-4dbc-dfb4-082dbf004635"
   },
   "outputs": [],
   "source": [
    "vectors.closest_words(words[\"book\"])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 279,
     "status": "ok",
     "timestamp": 1696397019471,
     "user": {
      "displayName": "Hai Yan (Helen) Lu",
      "userId": "06999938304037686402"
     },
     "user_tz": -660
    },
    "id": "5_mhGCqiWvov",
    "outputId": "581f96b9-e840-462c-be83-30c8bfb9899d"
   },
   "outputs": [],
   "source": [
    "words[\"paris\"] - words[\"france\"] + words[\"england\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGR0-tMJWvov"
   },
   "source": [
    "You can also input one vector and get its closet word by `closest_word()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 1228,
     "status": "ok",
     "timestamp": 1696397023421,
     "user": {
      "displayName": "Hai Yan (Helen) Lu",
      "userId": "06999938304037686402"
     },
     "user_tz": -660
    },
    "id": "FKkRkKchWvov",
    "outputId": "4bfb33ad-1845-4a83-a52f-ffd4b3c5ae66"
   },
   "outputs": [],
   "source": [
    "vectors.closest_word(words[\"paris\"] - words[\"france\"] + words[\"england\"])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

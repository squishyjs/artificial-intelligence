{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xvXwpe-Wvog"
   },
   "source": [
    "## Lab 8 NLP basics\n",
    "\n",
    "In this notebook, you will learn how the basic NLP methods work. The methods include:\n",
    "* Extract words from a given text using word-tokenisation\n",
    "* Create n-grams from files in a given directory,\n",
    "* Generate word meaning vector using a Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5026,
     "status": "ok",
     "timestamp": 1696396864859,
     "user": {
      "displayName": "Hai Yan (Helen) Lu",
      "userId": "06999938304037686402"
     },
     "user_tz": -660
    },
    "id": "m95Qb3ZDWvoi",
    "outputId": "918d0643-c009-4bfa-a691-42fa2ec6d50d"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'nltk' has no attribute 'internals'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpopular\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\John\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\downloader.py:777\u001b[0m, in \u001b[0;36mDownloader.download\u001b[1;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error, print_error_to)\u001b[0m\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(s, prefix2\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    769\u001b[0m     print_to(\n\u001b[0;32m    770\u001b[0m         textwrap\u001b[38;5;241m.\u001b[39mfill(\n\u001b[0;32m    771\u001b[0m             s,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    774\u001b[0m         )\n\u001b[0;32m    775\u001b[0m     )\n\u001b[1;32m--> 777\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincr_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfo_or_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Error messages\u001b[39;49;00m\n\u001b[0;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mErrorMessage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    780\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\John\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\downloader.py:629\u001b[0m, in \u001b[0;36mDownloader.incr_download\u001b[1;34m(self, info_or_id, download_dir, force)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;66;03m# Look up the requested collection or package.\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 629\u001b[0m     info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info_or_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfo_or_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m ErrorMessage(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minfo_or_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\John\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\downloader.py:603\u001b[0m, in \u001b[0;36mDownloader._info_or_id\u001b[1;34m(self, info_or_id)\u001b[0m\n\u001b[0;32m    601\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_info_or_id\u001b[39m(\u001b[38;5;28mself\u001b[39m, info_or_id):\n\u001b[0;32m    602\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(info_or_id, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 603\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfo_or_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    604\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    605\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m info_or_id\n",
      "File \u001b[1;32mc:\\Users\\John\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\downloader.py:1009\u001b[0m, in \u001b[0;36mDownloader.info\u001b[1;34m(self, id)\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minfo\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mid\u001b[39m):\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the ``Package`` or ``Collection`` record for the\u001b[39;00m\n\u001b[0;32m   1008\u001b[0m \u001b[38;5;124;03m    given item.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1009\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1010\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_packages:\n\u001b[0;32m   1011\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_packages[\u001b[38;5;28mid\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\John\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\downloader.py:951\u001b[0m, in \u001b[0;36mDownloader._update_index\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_url \u001b[38;5;241m=\u001b[39m url \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_url\n\u001b[0;32m    950\u001b[0m \u001b[38;5;66;03m# Download the index file.\u001b[39;00m\n\u001b[1;32m--> 951\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minternals\u001b[49m\u001b[38;5;241m.\u001b[39mElementWrapper(\n\u001b[0;32m    952\u001b[0m     ElementTree\u001b[38;5;241m.\u001b[39mparse(urlopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_url))\u001b[38;5;241m.\u001b[39mgetroot()\n\u001b[0;32m    953\u001b[0m )\n\u001b[0;32m    954\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_timestamp \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    956\u001b[0m \u001b[38;5;66;03m# Build a dictionary of packages.\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'nltk' has no attribute 'internals'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"popular\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 941,
     "status": "ok",
     "timestamp": 1696396897666,
     "user": {
      "displayName": "Hai Yan (Helen) Lu",
      "userId": "06999938304037686402"
     },
     "user_tz": -660
    },
    "id": "Lq-muOTiWvok"
   },
   "outputs": [],
   "source": [
    "from heading import *\n",
    "from nltk.tokenize import *\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-R70xS7Wvol"
   },
   "source": [
    "## Tokenisation\n",
    "Tokenisation is to split a text into meaningful units. Sentence tokenization will produce tokens as sentences and Word tokenization is to produce tokens as words.\n",
    "\n",
    "Here you will try an example of Sentence tokenization and one example of Word tokenization.\n",
    "\n",
    "First you need to define a text that needs to be tokenised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 270,
     "status": "ok",
     "timestamp": 1696396905094,
     "user": {
      "displayName": "Hai Yan (Helen) Lu",
      "userId": "06999938304037686402"
     },
     "user_tz": -660
    },
    "id": "7rOoukZEWvol"
   },
   "outputs": [],
   "source": [
    "EXAMPLE_TEXT = \"Hello Mr. Smith, how are you doing today? The weather is great, and Python is awesome. The sky is pinkish-blue. You shouldn't eat cardboard.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIH65f_sWvom"
   },
   "source": [
    "Then you use the method `sent_tokenize()` to separate the text into sentences and print out the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 280,
     "status": "ok",
     "timestamp": 1696396908659,
     "user": {
      "displayName": "Hai Yan (Helen) Lu",
      "userId": "06999938304037686402"
     },
     "user_tz": -660
    },
    "id": "Bt_YKFJOWvon",
    "outputId": "df3c85fb-5248-434a-e4a2-063dd6dab186"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Mr. Smith, how are you doing today?', 'The weather is great, and Python is awesome.', 'The sky is pinkish-blue.', \"You shouldn't eat cardboard.\"]\n"
     ]
    }
   ],
   "source": [
    "sent_tokenize_result = sent_tokenize(EXAMPLE_TEXT)\n",
    "print(sent_tokenize_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WiIzbAQqWvoo"
   },
   "source": [
    "You can also use the method `word_tokenize()` to separate the text into words and print out the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 402,
     "status": "ok",
     "timestamp": 1696396912120,
     "user": {
      "displayName": "Hai Yan (Helen) Lu",
      "userId": "06999938304037686402"
     },
     "user_tz": -660
    },
    "id": "3RxepZ0pWvoo",
    "outputId": "13f5c466-4f33-4195-92a6-f91f03e824bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'Python', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', \"n't\", 'eat', 'cardboard', '.']\n"
     ]
    }
   ],
   "source": [
    "word_tokenize_result = word_tokenize(EXAMPLE_TEXT)\n",
    "print(word_tokenize_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 531,
     "status": "ok",
     "timestamp": 1696396925980,
     "user": {
      "displayName": "Hai Yan (Helen) Lu",
      "userId": "06999938304037686402"
     },
     "user_tz": -660
    },
    "id": "hc0wm9iiWvop",
    "outputId": "91452719-fe4f-42fe-d184-b705914efa83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All punctuation: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "all_punctuation = string.punctuation\n",
    "print(f\"All punctuation: {all_punctuation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 294,
     "status": "ok",
     "timestamp": 1696396930143,
     "user": {
      "displayName": "Hai Yan (Helen) Lu",
      "userId": "06999938304037686402"
     },
     "user_tz": -660
    },
    "id": "07JuOjK4Wvoq"
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\John/nltk_data'\n    - 'c:\\\\Users\\\\John\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data'\n    - 'c:\\\\Users\\\\John\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\John\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\John\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\John\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\John\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\John/nltk_data'\n    - 'c:\\\\Users\\\\John\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data'\n    - 'c:\\\\Users\\\\John\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\John\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\John\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# You can configure for the language you need. In this example, you can use 'English'\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m stops \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\John\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32mc:\\Users\\John\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32mc:\\Users\\John\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\John\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\John/nltk_data'\n    - 'c:\\\\Users\\\\John\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data'\n    - 'c:\\\\Users\\\\John\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\John\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\John\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# You can configure for the language you need. In this example, you can use 'English'\n",
    "stops = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1696396933145,
     "user": {
      "displayName": "Hai Yan (Helen) Lu",
      "userId": "06999938304037686402"
     },
     "user_tz": -660
    },
    "id": "So0AA9p0Wvoq",
    "outputId": "775e9a62-0b5c-40f0-dc13-89cace58d237"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stops' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Filter the stop words and punctuations etc by removing the stop words and punctuations in the word list\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m removales \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mstops\u001b[49m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mlist\u001b[39m(all_punctuation)\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m wordsWithoutStopWords \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m word_tokenize_result:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stops' is not defined"
     ]
    }
   ],
   "source": [
    "#Filter the stop words and punctuations etc by removing the stop words and punctuations in the word list\n",
    "removales = list(stops)+list(all_punctuation)+list(\"n't\")\n",
    "wordsWithoutStopWords = []\n",
    "\n",
    "for w in word_tokenize_result:\n",
    "    w_lower = w.lower()\n",
    "    if (w_lower not in stops) and (w_lower!=\"n't\") and (w_lower not in all_punctuation):\n",
    "        wordsWithoutStopWords.append(w)\n",
    "\n",
    "print(wordsWithoutStopWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3b4_KZ0vWvor"
   },
   "source": [
    "### N-grams\n",
    "\n",
    "A sequence of written items of length N is called an N-gram. The unigram (or 1-gram), bigram (or 2-gram), and trigram (or 3-gram) are sequences of one, two, and three items, respectively.\n",
    "\n",
    "In a character N-gram, the items are characters, and in a word N-gram the items are words. Here we practice to generate word N-grams for a given sentence in a simple application example.\n",
    "\n",
    "Example: Extract the number of occurrences of N-grams in a given document.\n",
    "\n",
    "First you need to specify two pieces of information:\n",
    "* the path to a folder containing the given document files and\n",
    "* the N in N-grams, which is the number of consecutive words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 270,
     "status": "ok",
     "timestamp": 1696396944479,
     "user": {
      "displayName": "Hai Yan (Helen) Lu",
      "userId": "06999938304037686402"
     },
     "user_tz": -660
    },
    "id": "rH67HDZEWvor"
   },
   "outputs": [],
   "source": [
    "file_directory = \"text_data/holmes\"\n",
    "num_words = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_N55P3zWvor"
   },
   "source": [
    "Then, you need to create an instance of `Ngrams` using the directory path and the N value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1696396947063,
     "user": {
      "displayName": "Hai Yan (Helen) Lu",
      "userId": "06999938304037686402"
     },
     "user_tz": -660
    },
    "id": "69wvwDhLWvor"
   },
   "outputs": [],
   "source": [
    "ngrams = Ngrams(file_directory,num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yd9HA2JOWvos"
   },
   "source": [
    "Now you can get frequencies of N-grams in the given corpus of documents. Here we display top 10 N-grams with their frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7614,
     "status": "ok",
     "timestamp": 1696396957243,
     "user": {
      "displayName": "Hai Yan (Helen) Lu",
      "userId": "06999938304037686402"
     },
     "user_tz": -660
    },
    "id": "_PFAUGoJWvos",
    "outputId": "e0a7f5af-d34b-4204-8caf-1395200ee993"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1158: ('of', 'the')\n",
      "879: ('in', 'the')\n",
      "521: ('it', 'was')\n",
      "498: ('to', 'the')\n",
      "463: ('it', 'is')\n",
      "457: ('i', 'have')\n",
      "405: ('that', 'i')\n",
      "378: ('at', 'the')\n",
      "370: ('and', 'i')\n",
      "332: ('and', 'the')\n"
     ]
    }
   ],
   "source": [
    "ngrams.top_term_frequencies(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iv9eE9a_Wvos"
   },
   "source": [
    "You can generate the n-grams from a given corpus, here is an example of getting 2-grams of a test string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 285,
     "status": "ok",
     "timestamp": 1696396962168,
     "user": {
      "displayName": "Hai Yan (Helen) Lu",
      "userId": "06999938304037686402"
     },
     "user_tz": -660
    },
    "id": "BKVRY3UhWvos",
    "outputId": "4f4819d1-bfa9-43b1-97a7-8a3aa0cd07b1"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wordsWithoutStopWords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ngrams_test \u001b[38;5;241m=\u001b[39m Counter(nltk\u001b[38;5;241m.\u001b[39mngrams(\u001b[43mwordsWithoutStopWords\u001b[49m,num_words))\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(ngrams_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'wordsWithoutStopWords' is not defined"
     ]
    }
   ],
   "source": [
    "ngrams_test = Counter(nltk.ngrams(wordsWithoutStopWords,num_words))\n",
    "print(ngrams_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 280,
     "status": "ok",
     "timestamp": 1696396976068,
     "user": {
      "displayName": "Hai Yan (Helen) Lu",
      "userId": "06999938304037686402"
     },
     "user_tz": -660
    },
    "id": "deDqfktUWvot",
    "outputId": "ec3a1146-445b-46bd-a296-5f82961de899"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ngrams_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ngram, freq \u001b[38;5;129;01min\u001b[39;00m \u001b[43mngrams_test\u001b[49m\u001b[38;5;241m.\u001b[39mmost_common(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfreq\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mngram\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ngrams_test' is not defined"
     ]
    }
   ],
   "source": [
    "for ngram, freq in ngrams_test.most_common(5):\n",
    "    print(f\"{freq}: {ngram}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1n83TqEWWvot"
   },
   "source": [
    "### Generate a word vector to represent the meaning of a word using a Word2Vec model\n",
    "\n",
    "Use a Word2vec model you can generate a word vector to represent the word meaning.\n",
    "\n",
    "First, you need to define the path of a text file `words.txt` representing the word2vec model.\n",
    "In the following example, the file is in a local folder `text_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 292,
     "status": "ok",
     "timestamp": 1696396981275,
     "user": {
      "displayName": "Hai Yan (Helen) Lu",
      "userId": "06999938304037686402"
     },
     "user_tz": -660
    },
    "id": "Cc4hZMwuWvot"
   },
   "outputs": [],
   "source": [
    "file_directory = \"text_data/words.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twMddEmwWvou"
   },
   "source": [
    "Then, you can create an instance of `Vectors` class by calling the method `Vectors(localtion-your-word2vec-model-file)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 2786,
     "status": "ok",
     "timestamp": 1696396986547,
     "user": {
      "displayName": "Hai Yan (Helen) Lu",
      "userId": "06999938304037686402"
     },
     "user_tz": -660
    },
    "id": "M8muGmh3Wvou"
   },
   "outputs": [],
   "source": [
    "vectors = Vectors(file_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPvG9xpFWvou"
   },
   "source": [
    "Lastly, you can get the word model by calling the method `words` through the instance of `Vectors` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 277,
     "status": "ok",
     "timestamp": 1696397004292,
     "user": {
      "displayName": "Hai Yan (Helen) Lu",
      "userId": "06999938304037686402"
     },
     "user_tz": -660
    },
    "id": "Iig2_SKnWvou"
   },
   "outputs": [],
   "source": [
    "words = vectors.words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jlZaObdWvou"
   },
   "source": [
    "The elements in a word vector don't mean much. However, you can use the word vector to do the following things:\n",
    "* find out the distance between two words,\n",
    "* find out the closet of a word, and\n",
    "* find out the relationship between related words\n",
    "\n",
    "Now you can see some examples of word vectors and how to use them.\n",
    "\n",
    "Here are some examples:\n",
    "* word vector of \"city\"\n",
    "* disctance between \"city\" and \"book\"\n",
    "* top 10 closest words to word \"book\"\n",
    "* calculate a new word vector from \"paris\" - \"france\" + \"england\" and find the closest word to this new word\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 390,
     "status": "ok",
     "timestamp": 1696397007908,
     "user": {
      "displayName": "Hai Yan (Helen) Lu",
      "userId": "06999938304037686402"
     },
     "user_tz": -660
    },
    "id": "EWiCYPQ_Wvou",
    "outputId": "39c7306c-0686-4a89-810c-6dce5b61922c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.231087, -0.238098,  0.584713, -0.524351,  0.40278 ,  0.148448,\n",
       "        0.386096, -0.493994, -0.198922, -0.411161,  0.556962,  0.220978,\n",
       "       -0.304637, -0.499713, -0.092555,  0.262613,  0.752704,  0.463667,\n",
       "        0.054477,  0.155809, -0.195134, -0.009269,  0.378139, -0.651306,\n",
       "       -0.029372, -0.563472,  0.024709,  0.366842, -0.476904, -0.42565 ,\n",
       "       -0.094642, -0.052822,  0.124612,  0.296046, -0.244881,  0.195957,\n",
       "        0.223666,  0.064116,  0.577874,  0.083096, -0.378262,  0.196044,\n",
       "       -0.220993, -0.630213, -0.311214,  0.435611,  0.351486,  0.342794,\n",
       "       -0.229961, -0.157521,  0.204315,  0.253944, -0.562277,  0.534482,\n",
       "       -0.4158  ,  0.120161,  0.649395, -0.227012, -0.130488, -0.332326,\n",
       "        0.691952, -0.400436,  0.410125,  0.026237, -0.408483,  0.188236,\n",
       "        0.130957, -0.320686,  0.225932, -0.171665, -0.335107, -0.009982,\n",
       "        0.680831, -0.023788, -0.165798,  0.345986, -0.232295,  0.021137,\n",
       "        0.08515 , -0.24387 , -0.142469, -0.058325,  0.086046, -0.173068,\n",
       "        0.198108,  0.009103,  0.381725,  0.095911,  0.317972, -0.10012 ,\n",
       "        0.143178,  0.106724, -0.419844, -0.175785, -0.251805,  0.211927,\n",
       "        0.411175,  0.317378,  0.450316, -0.252661])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[\"city\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7_i5qYHWvov"
   },
   "source": [
    "You can calculate the vectors' distance between two different words by `distance()` method in `Vectors()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 298,
     "status": "ok",
     "timestamp": 1696397011301,
     "user": {
      "displayName": "Hai Yan (Helen) Lu",
      "userId": "06999938304037686402"
     },
     "user_tz": -660
    },
    "id": "jOBueniNWvov",
    "outputId": "fb4afbf7-b6b7-4117-c83f-cd5d2e9e82f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7886748166595728"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.distance(words[\"city\"], words[\"book\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PhjL0yyXWvov"
   },
   "source": [
    "You can also get the first 10 closest words by `closest_words()` method in `Vectors()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1667,
     "status": "ok",
     "timestamp": 1696397016161,
     "user": {
      "displayName": "Hai Yan (Helen) Lu",
      "userId": "06999938304037686402"
     },
     "user_tz": -660
    },
    "id": "VedrEuSgWvov",
    "outputId": "a2ff58d0-3f59-4dbc-dfb4-082dbf004635"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['book',\n",
       " 'books',\n",
       " 'essay',\n",
       " 'memoir',\n",
       " 'essays',\n",
       " 'novella',\n",
       " 'anthology',\n",
       " 'blurb',\n",
       " 'autobiography',\n",
       " 'audiobook']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.closest_words(words[\"book\"])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 279,
     "status": "ok",
     "timestamp": 1696397019471,
     "user": {
      "displayName": "Hai Yan (Helen) Lu",
      "userId": "06999938304037686402"
     },
     "user_tz": -660
    },
    "id": "5_mhGCqiWvov",
    "outputId": "581f96b9-e840-462c-be83-30c8bfb9899d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.874760e-01, -3.692510e-01,  3.075000e-01, -4.181100e-01,\n",
       "        5.498840e-01,  6.352010e-01, -4.005000e-03, -4.593450e-01,\n",
       "       -3.470230e-01,  1.659260e-01,  4.006800e-02,  4.340700e-01,\n",
       "       -8.286800e-01,  6.503120e-01, -3.537860e-01, -1.202400e-02,\n",
       "        6.322300e-02, -2.357520e-01,  2.798800e-01,  5.046000e-02,\n",
       "       -7.738000e-03,  5.448990e-01,  2.280140e-01, -1.064991e+00,\n",
       "        1.754930e-01,  1.058160e-01,  1.875940e-01,  4.105300e-02,\n",
       "        4.010000e-04, -2.287470e-01, -3.982840e-01,  2.528880e-01,\n",
       "       -3.941240e-01,  4.546850e-01,  1.792500e-02,  3.963680e-01,\n",
       "        7.318080e-01, -7.724200e-02, -3.517500e-02, -4.509550e-01,\n",
       "       -5.403160e-01,  1.168860e-01, -7.841200e-02, -3.544150e-01,\n",
       "       -3.480260e-01, -6.697200e-02,  1.598300e-02,  3.642560e-01,\n",
       "        3.123690e-01,  2.153870e-01,  2.149050e-01, -5.187360e-01,\n",
       "       -1.973870e-01,  1.257630e-01,  2.832430e-01, -1.324610e-01,\n",
       "        5.708990e-01, -4.308300e-02, -2.368190e-01, -6.645990e-01,\n",
       "        3.335200e-01,  8.106600e-02,  1.879650e-01,  2.218570e-01,\n",
       "       -1.834460e-01, -1.107440e-01,  3.757870e-01, -1.223300e-01,\n",
       "       -1.382620e-01,  5.167000e-03, -2.230740e-01,  6.199900e-02,\n",
       "        1.653300e-01,  2.063610e-01, -7.412900e-02, -8.160600e-02,\n",
       "        9.970800e-02, -3.268720e-01, -1.714880e-01,  2.338190e-01,\n",
       "        2.815180e-01, -2.800020e-01, -4.860870e-01,  1.948650e-01,\n",
       "        2.738100e-01,  3.391530e-01, -1.507030e-01,  5.565700e-02,\n",
       "        3.968890e-01,  3.452460e-01, -4.930660e-01,  2.507470e-01,\n",
       "        2.583530e-01,  2.747260e-01, -3.939120e-01,  2.056880e-01,\n",
       "        4.643710e-01,  5.855500e-02,  3.345120e-01, -1.149680e-01])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[\"paris\"] - words[\"france\"] + words[\"england\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGR0-tMJWvov"
   },
   "source": [
    "You can also input one vector and get its closet word by `closest_word()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 1228,
     "status": "ok",
     "timestamp": 1696397023421,
     "user": {
      "displayName": "Hai Yan (Helen) Lu",
      "userId": "06999938304037686402"
     },
     "user_tz": -660
    },
    "id": "FKkRkKchWvov",
    "outputId": "4bfb33ad-1845-4a83-a52f-ffd4b3c5ae66"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'london'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.closest_word(words[\"paris\"] - words[\"france\"] + words[\"england\"])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

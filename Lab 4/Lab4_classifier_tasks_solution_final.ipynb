{"cells":[{"cell_type":"markdown","metadata":{"id":"zXl1EynD2qzT"},"source":["# Lab 4  Solving a classification problem using techniques in Lec 4\n","Develop classifiers using the techniques covered in Lec 4 to predict the class label of a new Iris flower."]},{"cell_type":"markdown","metadata":{"id":"1q2YgP5l2qzT"},"source":["## Solving a classification problem\n","In order to solve a classification problem, the following steps need to be taken:\n","1. Find a suitable dataset\n","2. Define the input dataset\n","3. Explore the input dataset.\n","After exploration, it may be needed to pre-process the dataset in real-world classification problems, i.e., to clean and pre-process the input dataset to make it ready for model development. In this example, the dataset is clean so there is no need for pre-processing.\n","4. Form the training dataset and test dataset\n","5. Develop a classifier by training a machine learning model based on a chosen algorithm using the training dataset\n","6. Evaluate the classifier\n","7. Deploy the classifier to solve the target classification problem (not covered in this notebook)"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"dQ9xHd7J-dZ3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cd /content/drive/MyDrive/IntroAILab/Lab4"],"metadata":{"id":"R_U7NGDs-rRv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wqYlqRU_2qzU"},"source":["### Findnig a Dataset\n","\n","The dataset is from the Iris Plants Database. https://gist.github.com/curran/a08a1080b88344b0c8a7\n","\n","Relevant Information about this dataset is as follows:\n","This is perhaps the best known database to be found in the pattern recognition literature.  The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant.  One class is linearly separable from the other 2; the latter are NOT linearly separable from each other.\n","* Predicted attribute: class of iris plant.\n","* This is an exceedingly simple domain.\n","* Number of Instances: 150 (50 in each of three classes)\n","* Number of Attributes: 4 numeric, predictive attributes and the class\n","* Attribute Information:\n","-- sepal length in cm\n","-- sepal width in cm\n","-- petal length in cm\n","-- petal width in cm\n","-- class: Iris Setosa, Iris Versicolour and Iris Virginica\n","\n","The class is the target attribute or variable."]},{"cell_type":"markdown","source":["### Image samples of these three classes are shown in the following cells, respectively."],"metadata":{"id":"5FWf-UsRTDx6"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","img1 = plt.imread('Iris_Setosa.png')\n","img2 = plt.imread('Iris_Versicolour.png')\n","img3 = plt.imread('Iris_Virginica.png')\n","fig, axes = plt.subplots(1,3)\n","axes[0].imshow(img1)\n","axes[0].set_title(\"Iris Setosa\")\n","axes[0].axis('off')\n","axes[1].imshow(img2)\n","axes[1].set_title(\"Iris Versicolour\")\n","axes[1].axis('off')\n","axes[2].imshow(img3)\n","axes[2].set_title(\"Iris Virginica\")\n","axes[2].axis('off')\n","plt.show()"],"metadata":{"id":"dFgHGQAnWDLX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LhD2-AwqwaeK"},"source":["## Section 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"elMe1DZ-waeL"},"outputs":[],"source":["# Task\n","# Import pandas package\n","\n","\"Add your code here\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P_TQNV8_2qzc"},"outputs":[],"source":["# Solution\n","\n","# Import pandas package\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XY2gOsEO2qzb"},"outputs":[],"source":["# Task\n","# Load the dataset by reading the iris.csv file using read_csv() method and save the output as a dataframe\n","\n","\"Add your code here\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gjuSz_cnwaeN"},"outputs":[],"source":["#Solution\n","\n","iris = pd.read_csv('iris.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"poG_hBfS75C_"},"outputs":[],"source":["# Task\n","\n","# Take a look at the first 5 samples of iris using the method head() of a pandas dataframe\n","\"Add your code here\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pX3JqJHFwaeP"},"outputs":[],"source":["# Solution\n","\n","# Take a look at the first 5 samples of iris using the method head() of a pandas dataframe\n","iris.head(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uFWujX1AwaeN"},"outputs":[],"source":["# Task\n","\n","# import LabelEncoder from sklearn.preprocessing\n","\"Add your code here\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W63vuzoMwaeN"},"outputs":[],"source":["# Solution\n","\n","# import LabelEncoder from sklearn.preprocessing\n","from sklearn.preprocessing import LabelEncoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RvgvpBdS6K3B"},"outputs":[],"source":["# Task\n","\n","# Instantiate the encoder by creating an instance of LabelEnbcoder class and name it as 'le'\n","le = \"Add your code here\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EHN-AzjTwaeO"},"outputs":[],"source":["# Solution\n","\n","# Instantiate the encoder\n","le = LabelEncoder()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D0ZPJPgnwaeO"},"outputs":[],"source":["# Task\n","\n","# Convert categorical target values into numeric values using the method fit_transform() in the encoder 'le' by passing the target column\n","\"Add your code here\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dtL3ral6waeO"},"outputs":[],"source":["# Solution\n","\n","# Convert categorical target values into numeric values using the encoder 'le' by fitting the encoder and transforming the target column\n","iris['species'] = le.fit_transform(iris['species'])"]},{"cell_type":"code","source":["# Take a look at the first 5 samples of iris using the method head() of a dataframe again after converting the target values\n","iris.head(5)"],"metadata":{"id":"rdZrW28mb88L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SyGzdSpvwaeP"},"source":["## Section 2"]},{"cell_type":"markdown","metadata":{"id":"vBqO7UX12qzm"},"source":["### Form the training dataset and test dataset. Also take a few future samples from the test dataset\n","\n","#### Create the training and test datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X_elcBgY2qzp"},"outputs":[],"source":["# Task\n","\n","# Import train_test_split from sklearn.model.selection\n","\"Add your code here\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZvlXB9NXwaeQ"},"outputs":[],"source":["# Solution\n","\n","# Import train_test_split from sklearn.model.selection\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r0jiG8L6waeQ"},"outputs":[],"source":["# Task\n","\n","# Define the input features X as all columns except 'species' column and target variable y as the \"species\" column\n","X = iris.drop(\"Add your code here\", axis=1)\n","y = iris[\"Add your code here\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nefmr2OOwaeQ"},"outputs":[],"source":["# Solution\n","\n","# Define the input features X as all columns except 'species' column and target variable y as the \"species\" column\n","X = iris.drop('species', axis=1)\n","y = iris['species']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2B28ixhM2qzp"},"outputs":[],"source":["# Task\n","\n","# Using train_test_split function to split the dataset into the training and test datasets, the percentage of samples in the test dataset is 20% and the seed for the random number generator is 42\n","\n","X_train, X_test, y_train, y_test = \"Add your code here\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jZoUigyowaeR"},"outputs":[],"source":["# Solution\n","\n","# Using train_test_split function to split the dataset into the training and test datasets, the percentage of samples in the test dataset is 20% and the seed for the random number generator is 42 (Ensure that the data split is the same every time the code is run with that seed)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mKbBnh1H2qzr"},"outputs":[],"source":["# Task\n","\n","# Make a futureSample test dataset\n","# Take two samples from the dataset as the future data samples, called futureSample_X, and futureSample_y,\n","# as the inputs from the real-world cases when the classifier is deployed.\n","\n","# Get the last two samples from the test  to be the future data samples\n","futureSample_X = \"Add your code here\"\n","futureSample_y = \"Add your code here\"\n","\n","# Remove the last two samples from the test dataset\n","X_test = \"Add your code here\"\n","y_test = \"Add your code here\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uHfsjkpqwaeR"},"outputs":[],"source":["# Solution\n","\n","# Make a futureSample test dataset\n","# Take two samples from the dataset as the future data samples, called futureSample_X, and futureSample_y,\n","# as the inputs from the real-world cases when the classifier is deployed.\n","\n","# Get the last two samples from the test  to be the future data samples\n","futureSample_X = X_test[-2:]\n","futureSample_y = y_test[-2:]\n","\n","# Remove the last two samples from the test dataset\n","X_test = X_test[:-2]\n","y_test = y_test[:-2]"]},{"cell_type":"markdown","metadata":{"id":"IXxMYFKnwaeR"},"source":["## Section 3"]},{"cell_type":"markdown","metadata":{"id":"7XO6cnAa2qzp"},"source":["### Explore the training and test datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CFNK0e7AwaeS"},"outputs":[],"source":["# import matplotlib.pyplot as plt\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G7t-l6lQ2qzq"},"outputs":[],"source":["# Example\n","\n","# Plot the samples in the training dataset (use the first two attributes)\n","plt.figure(figsize=(8, 6))\n","plt.scatter(X_train['sepal.length'], X_train['sepal.width'], c=y_train, cmap=plt.cm.Set1, edgecolor=\"k\")\n","plt.xlabel(\"Sepal length\")\n","plt.ylabel(\"Sepal width\")\n","plt.title(\"Training data (sepal attributes)\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AZDD5xphwaeS"},"outputs":[],"source":["# Using the example of plotting the samples in the training dataset (use the first two attributes), plot the samples in the training dataset (use the last two attributes)\n","plt.figure(figsize=(8, 6))\n","plt.scatter(X_train['petal.length'], X_train['petal.width'], c=y_train, cmap=plt.cm.Set1, edgecolor=\"k\")\n","plt.xlabel(\"Petal length\")\n","plt.ylabel(\"Petal width\")\n","plt.title(\"Training data (petal attributes)\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zZKSe-R5waeS"},"outputs":[],"source":["# Plot the samples in the test dataset (use the first two attributes)\n","plt.figure(figsize=(8, 6))\n","plt.scatter(X_test['sepal.length'], X_test['sepal.width'], c=y_test, cmap=plt.cm.Set1, edgecolor=\"k\")\n","plt.xlabel(\"Sepal length\")\n","plt.ylabel(\"Sepal width\")\n","plt.title(\"Test data (sepal attributes)\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-lXn8IdSwaeT"},"outputs":[],"source":["# Using the example of plotting the samples in the test dataset (use the first two attributes), plot the samples in the test dataset (use the last two attributes)\n","plt.figure(figsize=(8, 6))\n","plt.scatter(X_test['petal.length'], X_test['petal.width'], c=y_test, cmap=plt.cm.Set1, edgecolor=\"k\")\n","plt.xlabel(\"Petal length\")\n","plt.ylabel(\"Petal width\")\n","plt.title(\"Test data (petal attributes)\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"bryKmmQXwaeT"},"source":["## Section 4"]},{"cell_type":"markdown","metadata":{"id":"DS_0Q4tm2qzr"},"source":["### Train a classification model by using a chosen learning algorithm with the trainig dataset.\n","\n","***First***, take an integer in the range of 1-5 from the user and save it to `model_option` to represent a model.\n","\n","model_option: 1- decision tree, 2- Random forest, 3-logistic regression, 4-K nearest neighbours, or 5 -- Support vector classifier\n","\n","If a user enters a number  >5, print out a message \"invalid option number. Try again\".\n","\n","***Second***, train the model using the train dataset\n","\n","***Third***, evaluate the model using the test dataset\n","\n","***Lastly***, consume the model using the futureSample test dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b24HmZrm2qzs"},"outputs":[],"source":["# Tasks\n","model_option = int(input(\"Choose one model from the following: 1- decision tree, 2- Random forest, 3- logistic regression, 4-K nearest neighbours, or 5- Support vector classifier \\n your choice is: \"))\n","if (model_option == 1):\n","    #  Import `DecisionTreeClassifier` from `sklearn.tree`.\n","    \"Add your code here\"\n","    \"\"\"\n","    Call the constructor `DecisionTreeClassifier()` to create a decision tree classifier object, name it as 'model', by passing the following key parameters:\n","    (i) \"criterion\", which represents the measure used in expanning the decision tree.\n","         There are a number of options for this parameter, such as `gini` (Gini impurity) and `entropy` or 'log_loss(information gain).\n","         By default, it is `gini`. We use `entropy`.\n","    (ii) \"splitter\": The strategy used to choose the split at each node.\n","          Supported strategies are “best” to choose the best split and “random” to choose the best random split. Default is ”best”.\n","    (iii) \"max_depth\": Maximum depth of the tree. If None, nodes are expanded until all leaves are pure or contains fewer than `min_samples_split`\n","    (iv) \"min_sample_split\": Minimum number of samples required to split an internal node, default is 2\n","    (v) \"min_samples_leaf\" Minimum number of samples required to be at a leaf node. default is 1.\n","    (vi) \"max_features\": number of features to consider when looking for the best split, default is None (max_features=n_features).\n","        Other values are 'sqrt' (max_features=sqrt(n_features)) and \"log2” (max_features=log2(n_features))\n","    (vii) \"random_state\": Controls the randomness of the estimator. default is None.\n","          To obtain a deterministic behaviour during fitting, random_state has to be fixed to an integer, say random_state = 42.\n","    \"\"\"\n","    model = \"Add your code here\" # the parameters can be omitted if you want to use their default values\n","    # Train this model using the training data set (train_data, train_label).\n","    \"Add your code here\"\n","elif (model_option == 2):\n","    # Import RandomForestClassifier from sklearn.ensemble.\n","    \"Add your code here\"\n","    \"\"\"\n","    Call the constructor RandomForestClassifier() to create a RandomForestClassifier object, name it as 'model'.\n","    Passing the following parameters:\n","    (i) 'n_estimator': Number of trees in the forest, default is 100\n","    (ii) 'criterion', which represents the measure used in expanning the decision tree.\n","         There are a number of options for this parameter, such as `gini` (Gini impurity) and `entropy` or 'log_loss(information gain).\n","         By default, it is `gini`. We use `entropy`.\n","    (iii) 'max_depth': Maximum depth of the trees,\n","         default is None (nodes are expanded until all leaves are pure or\n","         until they contain fewer than 'min_sample_split' samples)\n","    (iv) 'min_samples_split': Minum number of samples required to split an internal node, default is 2\n","    (v) 'min_samples_leaf': Minum number of samples required to be at a leaf node, default is 1\n","    (vi) 'max_features': number of features to consider when looking for the best split, default is None (max_features=n_features).\n","        Other values are 'sqrt' (max_features=sqrt(n_features)) and \"log2” (max_features=log2(n_features))\n","    (vii) 'bootstrap': Whether to use bootstrap samples when building trees, default is 'True'.\n","          If 'False', the entire dataset is used to build each tree, which may lead to overfitting.\n","    (viii) 'oob_score': Whether to use out-of-bag samples to estimate the generalisation accuracy.\n","          default value is 'False'. If 'True', an unbiased estimate of the model performance is provided.\n","    \"\"\"\n","    model = \"Add your code here\" # the parameters can be omitted if you want to use their default values\n","    # Train this model using the training dataset (train_data, train_label).\n","    \"Add your code here\"\n","elif (model_option == 3):\n","    # Import LogisticRegression from sklearn.linear_model.\n","    \"Add your code here\"\n","    \"\"\"\n","    Call the constructor LogisticRegression() to create a LogisticRegresion object, name it as 'model', by passing the following parameters:.\n","    (i) 'penalty':   Specify the norm of the penalty.\n","         Possible values: None (no penalty); 'l2' (add a L2 penalty term);\n","         'l1' (add a L1 penalty term); 'elasticnet' (both L1 and L2 penalty terms are added).  Default is l2\n","    (ii) 'dual': Dual (constrained) or primal (regularized) formulation. Default is False.\n","         Dual formulation is only implemented for l2 penalty with liblinear solver.\n","    (iii) 'random_state': Used when solver == ‘sag’, ‘saga’ or ‘liblinear’ to shuffle the data. Default is None\n","\n","    (iv)'solver': Algorithm to use in the optimization problem.\n","         possible values: ‘lbfgs’, ‘liblinear’, ‘newton-cg’, ‘newton-cholesky’, ‘sag’, and ‘saga’. Default is ’lbfgs’\n","         To choose a solver, you might want to consider the following aspects\n","         (more info see https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html):\n","          -- For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and ‘saga’ are faster for large ones;\n","          -- For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ handle multinomial loss;\n","          -- ‘liblinear’ and ‘newton-cholesky’ can only handle binary classification by default.\n","          -- To apply a one-versus-rest scheme for the multiclass setting one can wrapt it with the OneVsRestClassifier.\n","          -- ‘newton-cholesky’ is a good choice for n_samples >> n_features, especially with one-hot encoded categorical features with rare categories.\n","             Be aware that the memory usage of this solver has a quadratic dependency on n_features because it explicitly computes the Hessian matrix.\n","    (v) 'max_iter': Maximum number of iterations taken for the solvers to converge. Default is 100.\n","    \"\"\"\n","    model = \"Add your code here\" # the parameters can be omitted if you want to use their default values\n","    # Train this model using the training dataset (train_data, train_label).\n","    \"Add your code here\"\n","elif (model_option == 4):\n","    # Import `KNeighborsClassifier` from sklearn.neighbors.\n","    \"Add your code here\"\n","    \"\"\"\n","    Call the constructor `KNeighborsClassifier()` to create a KNN classifier object, name it as 'model', by passing the following key parameters:.\n","    Set the number of neighbors as 3\n","    (i) 'n_neighbors': Number of neighbors to use by default for kneighbors queries. Default is 5.\n","    (ii) 'weights': Weight function used in prediction. Possible values are ‘uniform’ (uniform weights),\n","         ‘distance’ (weight points by the inverse of their distance) and\n","         [callable] (a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights). Default = 'uniform'\n","    (iii) 'algorithm': Algorithm used to compute the nearest neighbors. Possible values are ‘auto’ (attempt to decide the most appropriate algorithm based on the values passed to fit method),\n","          ‘ball_tree’ (use BallTree), ‘kd_tree’ (use KDTree), and ‘brute’ (use a brute-force search). Default is’auto’.\n","    \"\"\"\n","    model = \"Add your code here\"  # the parameters can be omitted if you want to use their default values\n","    # Train this model using the training dataset (train_data, train_label).\n","    \"Add your code here\"\n","elif (model_option == 5):\n","    # Import SVC from sklearn.svm.\n","    \"Add your code here\"\n","    \"\"\"\n","    Call the constructor SVC() to create a SVC object, name it as 'model', by passing the following key parameters:\n","\n","    (i) 'C': Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive.\n","        Default=1.0\n","    (ii) 'kernel': Specifies the kernel type to be used in the algorithm. possible values are ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’ and ‘precomputed’.\n","         Default is ’rbf’.\n","    (iii) 'degree': Degree of the polynomial kernel function (‘poly’). Must be non-negative. Ignored by all other kernels. Default=3\n","    (iv) 'gamma': Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’.\n","          Possible values are ‘scale’ (1 / (n_features * X.var())) and ‘auto’ (1 / n_features).\n","          Default is ’scale’.\n","    (v) 'probability': Whether to enable probability estimates. Default is False.\n","         This must be enabled prior to calling fit. If use 'True', it will slow down that method as it internally uses 5-fold cross-validation.\n","    (vi) 'max_iter': Hard limit on iterations within solver, or -1 for no limit. Default is -1.\n","    (vii) 'decision_function_shape': Whether to return a one-vs-rest (‘ovr’) decision function of shape (n_samples, n_classes) as all other classifiers,\n","          or the original one-vs-one (‘ovo’) decision function of libsvm which has shape (n_samples, n_classes * (n_classes - 1) / 2).\n","          Default is ’ovr’. The parameter is ignored for binary classification.\n","    (viii) 'random_state': Controls the pseudo random number generation for shuffling the data for probability estimates.\n","          Ignored when probability is False. Default is None.\n","          Pass an int for reproducible output across multiple function calls.\n","    \"\"\"\n","    model = \"Add your code here\"  # the parameters can be omitted if you want to use their default values\n","    # Train this model using the training dataset (train_data, train_label).\n","    \"Add your code here\"\n","else: print(\"invalid option number. Try again\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0d9wN2Yk2qzs"},"outputs":[],"source":["# Solution\n","model_option = int(input(\"Choose one model from the following: 1- decision tree, 2- Random forest, 3- logistic regression, 4-K nearest neighbours, or 5- Support vector classifier \\n your choice is: \"))\n","\n","if model_option == 1:\n","    from sklearn.tree import DecisionTreeClassifier\n","    \"\"\"\n","    Call the constructor `DecisionTreeClassifier()` to create a decision tree classifier object, name it as 'model', by passing the following key parameters:\n","    (i) \"criterion\", which represents the measure used in expanning the decision tree.\n","         There are a number of options for this parameter, such as `gini` (Gini impurity) and `entropy` or 'log_loss(information gain).\n","         By default, it is `gini`. We use `entropy`.\n","    (ii) \"splitter\": The strategy used to choose the split at each node.\n","          Supported strategies are “best” to choose the best split and “random” to choose the best random split. Default is ”best”.\n","    (iii) \"max_depth\": Maximum depth of the tree. If None, nodes are expanded until all leaves are pure or contains fewer than `min_samples_split`\n","    (iv) \"min_sample_split\": Minimum number of samples required to split an internal node, default is 2\n","    (v) \"min_samples_leaf\" Minimum number of samples required to be at a leaf node. default is 1.\n","    (vi) \"max_features\": number of features to consider when looking for the best split, default is None (max_features=n_features).\n","        Other values are 'sqrt' (max_features=sqrt(n_features)) and \"log2” (max_features=log2(n_features))\n","    (vii) \"random_state\": Controls the randomness of the estimator. default is None.\n","          To obtain a deterministic behaviour during fitting, random_state has to be fixed to an integer, say random_state = 42.\n","    \"\"\"\n","    model = DecisionTreeClassifier(criterion = 'entropy', max_depth = None, min_samples_split=2)\n","    model.fit(X_train, y_train)\n","\n","elif model_option == 2:\n","    from sklearn.ensemble import RandomForestClassifier\n","    \"\"\"\n","    Call the constructor RandomForestClassifier() to create a RandomForestClassifier object, name it as 'model'.\n","    Passing the following parameters:\n","    (i) n_estimator: Number of trees in the forest, default is 100\n","    (ii) \"criterion\", which represents the measure used in expanning the decision tree.\n","         There are a number of options for this parameter, such as `gini` (Gini impurity) and `entropy` or 'log_loss(information gain).\n","         By default, it is `gini`. We use `entropy`.\n","    (iii) max_depth: Maximum depth of the trees,\n","         default is None (nodes are expanded until all leaves are pure or\n","         until they contain fewer than 'min_sample_split' samples)\n","    (iv) min_samples_split: Minum number of samples required to split an internal node, default is 2\n","    (v) min_samples_leaf: Minum number of samples required to be at a leaf node, default is 1\n","    (vi) max_features: number of features to consider when looking for the best split, default is None (max_features=n_features).\n","        Other values are 'sqrt' (max_features=sqrt(n_features)) and \"log2” (max_features=log2(n_features))\n","    (vii) 'bootstrap: Whether to use bootstrap samples when building trees, default is 'True'.\n","          If 'False', the entire dataset is used to build each tree, which may lead to overfitting.\n","    (viii) oob_score: Whether to use out-of-bag samples to estimate the generalisation accuracy.\n","          default value is 'False'. If 'True', an unbiased estimate of the model performance is provided.\n","    \"\"\"\n","    model = RandomForestClassifier(n_estimators=100, criterion='entropy')\n","    model.fit(X_train, y_train)\n","\n","elif model_option == 3:\n","    from sklearn.linear_model import LogisticRegression\n","    \"\"\"\n","    Call the constructor LogisticRegression() to create a LogisticRegresion object, name it as 'model', by passing the following parameters:.\n","    (i) 'penalty':   Specify the norm of the penalty.\n","         Possible values: None (no penalty); 'l2' (add a L2 penalty term);\n","         'l1' (add a L1 penalty term); 'elasticnet' (both L1 and L2 penalty terms are added).  Default is l2\n","    (ii) 'dual': Dual (constrained) or primal (regularized) formulation. Default is False.\n","         Dual formulation is only implemented for l2 penalty with liblinear solver.\n","    (iii) 'random_state': Used when solver == ‘sag’, ‘saga’ or ‘liblinear’ to shuffle the data. Default is None\n","\n","    (iv)'solver': Algorithm to use in the optimization problem.\n","         possible values: ‘lbfgs’, ‘liblinear’, ‘newton-cg’, ‘newton-cholesky’, ‘sag’, and ‘saga’. Default is ’lbfgs’\n","         To choose a solver, you might want to consider the following aspects\n","         (more info see https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html):\n","          -- For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and ‘saga’ are faster for large ones;\n","          -- For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ handle multinomial loss;\n","          -- ‘liblinear’ and ‘newton-cholesky’ can only handle binary classification by default.\n","          -- To apply a one-versus-rest scheme for the multiclass setting one can wrapt it with the OneVsRestClassifier.\n","          -- ‘newton-cholesky’ is a good choice for n_samples >> n_features, especially with one-hot encoded categorical features with rare categories.\n","             Be aware that the memory usage of this solver has a quadratic dependency on n_features because it explicitly computes the Hessian matrix.\n","    (v) 'max_iter': Maximum number of iterations taken for the solvers to converge. Default is 100.\n","    \"\"\"\n","    model = LogisticRegression(max_iter=1000)  # Increase max_iter if the algorithm doesn't converge\n","    model.fit(X_train, y_train)\n","\n","elif model_option == 4:\n","    from sklearn.neighbors import KNeighborsClassifier\n","    \"\"\"\n","    Call the constructor `KNeighborsClassifier()` to create a KNN classifier object, name it as 'model', by passing the following key parameters:.\n","    Set the number of neighbors as 3\n","    (i) 'n_neighbors': Number of neighbors to use by default for kneighbors queries. Default is 5.\n","    (ii) 'weights': Weight function used in prediction. Possible values are ‘uniform’ (uniform weights),\n","         ‘distance’ (weight points by the inverse of their distance) and\n","         [callable] (a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights).\n","    (iii) 'algorithm': Algorithm used to compute the nearest neighbors. Possible values are ‘auto’ (attempt to decide the most appropriate algorithm based on the values passed to fit method),\n","          ‘ball_tree’ (use BallTree), ‘kd_tree’ (use KDTree), and ‘brute’ (use a brute-force search). Default is’auto’.\n","    \"\"\"\n","    model = KNeighborsClassifier(n_neighbors=3)\n","    model.fit(X_train, y_train)\n","\n","elif model_option == 5:\n","    from sklearn.svm import SVC\n","    \"\"\"\n","    Call the constructor SVC() to create a SVC object, name it as 'model', by passing the following key parameters:\n","\n","    (i) 'C': Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive.\n","        Default=1.0\n","    (ii) 'kernel': Specifies the kernel type to be used in the algorithm. possible values are ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’ and ‘precomputed’.\n","         Default is ’rbf’.\n","    (iii) 'degree': Degree of the polynomial kernel function (‘poly’). Must be non-negative. Ignored by all other kernels. Default=3\n","    (iv) 'gamma': Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’.\n","          Possible values are ‘scale’ (1 / (n_features * X.var())) and ‘auto’ (1 / n_features).\n","          Default is ’scale’.\n","    (v) 'probability': Whether to enable probability estimates. Default is False.\n","         This must be enabled prior to calling fit. If use 'True', it will slow down that method as it internally uses 5-fold cross-validation.\n","    (vi) 'max_iter': Hard limit on iterations within solver, or -1 for no limit. Default is -1.\n","    (vii) 'decision_function_shape': Whether to return a one-vs-rest (‘ovr’) decision function of shape (n_samples, n_classes) as all other classifiers,\n","          or the original one-vs-one (‘ovo’) decision function of libsvm which has shape (n_samples, n_classes * (n_classes - 1) / 2).\n","          Default is ’ovr’. The parameter is ignored for binary classification.\n","    (viii) 'random_state': Controls the pseudo random number generation for shuffling the data for probability estimates.\n","          Ignored when probability is False. Default is None.\n","          Pass an int for reproducible output across multiple function calls.\n","    \"\"\"\n","    model = SVC(gamma='auto',probability=True)\n","    model.fit(X_train, y_train)\n","\n","else:\n","    print(\"invalid option number. Try again\")"]},{"cell_type":"markdown","metadata":{"id":"vRFrG0WvwaeZ"},"source":["## Section 5"]},{"cell_type":"markdown","metadata":{"id":"JzsYme7BwaeZ"},"source":["### Evaluate a classification model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZYwzr9NU2qzu"},"outputs":[],"source":["# Import required packages for evaluating a classification model using the test dataset\n","from sklearn import metrics\n","from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n","from sklearn.metrics import accuracy_score"]},{"cell_type":"markdown","metadata":{"id":"wiQItu_S2qzt"},"source":["Test the classification model's performance using the method `predict()` to calculate the predicted values of test data and store the values in a variable, `solution_test`, and use a number of measures to evaluate the performance of this model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DTql8SX12qzu"},"outputs":[],"source":["# Task\n","# Predict the class labels of samples in the test dataset\n","y_pred = \"Add your code here\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"og3wRXgI2qz-"},"outputs":[],"source":["# Solution\n","# Predict the class labels of samples in the test dataset\n","y_pred = model.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DuWgK5C_2q0A"},"outputs":[],"source":["# Task\n","\n","# Calculate the accuracy of prediction of lables of test samples using the method accuracy_score()\n","score_test = \"Add your code here\"\n","print (score_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qBQHVRDBwaea"},"outputs":[],"source":["# Solution\n","\n","# Calculate the accuracy of prediction of lables of test samples using the method accuracy_score()\n","accuracy_test = accuracy_score(y_test, y_pred)\n","print (accuracy_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H5RG44wn2q0C"},"outputs":[],"source":["# Task\n","\n","# Calculate the precision of prediction of lables of test samples using the method precision_score() in metrics\n","precision_test = \"Add your code here\"\n","print (precision_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"efCydediwaea"},"outputs":[],"source":["# Solution\n","\n","# Calculate the precision of prediction of lables of test samples using the method precision_score() in metrics\n","precision_test = metrics.precision_score(y_test, y_pred, average='weighted')\n","print (precision_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1T0clo7q2q0D"},"outputs":[],"source":["# Task\n","\n","# calculate the recall of prediction of lables of test samples using the method recall_score() in metrics\n","recall_test = \"Add your code here\"\n","print (recall_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tnlni7nswaeb"},"outputs":[],"source":["# Solution\n","\n","# calculate the recall of prediction of lables of test samples using the method recall_score() in metrics\n","recall_test = metrics.recall_score(y_test, y_pred, average='weighted')\n","print (recall_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tsrb6Krs2q0D"},"outputs":[],"source":["# Task\n","\n","# Calculate the F1-score of prediction of lables of test samples using the method f1_score() in metrics\n","f1_test =  \"Add your code here\"\n","print(f1_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aQjxOcdmwaeb"},"outputs":[],"source":["# Solution\n","\n","# Calculate the F1-score of prediction of lables of test samples using the method f1_score() in metrics\n","f1_test = metrics.f1_score(y_test, y_pred, average='weighted')\n","print(f1_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_eCRxW102q0E"},"outputs":[],"source":["# Task\n","\n","# Display the confusion matrix using y_test and y_pred, the display lables are \"Setosa,\", \"Versicolour\", \"Virginica\".\n","fig,ax = plt.subplots(figsize=(5,4),dpi = 100)\n","cm = confusion_matrix( \"Add your code here\")\n","cmp = ConfusionMatrixDisplay(cm, display_labels = [ \"Add your code here\"])\n","cmp.plot(ax = ax);"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"USGFsbEfwaec"},"outputs":[],"source":["# Solution\n","\n","# Display the confusion matrix\n","fig,ax = plt.subplots(figsize=(5,4),dpi = 100)\n","cm = confusion_matrix(y_test, y_pred)\n","cmp = ConfusionMatrixDisplay(cm, display_labels = [\"Setosa,\", \"Versicolour\", \"Virginica\"])\n","cmp.plot(ax = ax);"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"joIJ9UUA2q0E"},"outputs":[],"source":["# Task\n","\n","# Present the overal report of the model's evaluation results using the method classification_report() in metrics\n","report = \"Add your code here\"\n","print(report)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1wudn5Lowaec"},"outputs":[],"source":["# Solution\n","\n","# Present the overal report of the model's evaluation results using the method classification_report() in metrics\n","report = metrics.classification_report(y_test, y_pred)\n","print(report)"]},{"cell_type":"markdown","metadata":{"id":"anzTOyf6waec"},"source":["## Section 6"]},{"cell_type":"markdown","metadata":{"id":"CZ-jXJHt2q0F"},"source":["### Predict the class labels for the data samples in the future sample set\n","Use the futureSample_X to simulate the new data items in a real-world application scenario. Use the method `predict()` to calculate the predicted values of futureSample data and store the values in a variable, `solution_validate`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"12NGae8J2q0F"},"outputs":[],"source":["# Task\n","# Predicted class lables the samples in the futureSample dataset\n","\n","solution_validate = \"Add your code here\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PHIjDZ3x2q0G"},"outputs":[],"source":["# Solution\n","\n","# Predicted class lables the samples in the futureSample dataset\n","solution_validate = model.predict(futureSample_X)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m1BaH08g2q0G"},"outputs":[],"source":["# Task\n","\n","# Calculate the prediction accuracy\n","score_validate = \"Add your code here\"\n","print (score_validate)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8zqR7NrHwaed"},"outputs":[],"source":["# Solution\n","\n","# Calculate the prediction accuracy\n","score_validate = accuracy_score(solution_validate, futureSample_y)\n","print (score_validate)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"pycharm":{"stem_cell":{"cell_type":"raw","metadata":{"collapsed":false},"source":[]}},"vp":{"vp_config_version":"1.0.0","vp_menu_width":273,"vp_note_display":false,"vp_note_width":0,"vp_position":{"width":278},"vp_section_display":false,"vp_signature":"VisualPython"}},"nbformat":4,"nbformat_minor":0}
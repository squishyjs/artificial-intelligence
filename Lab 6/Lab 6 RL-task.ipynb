{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 6 Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains four parts: the first part is the definition of a Markov decision process (MDP) which includes the state space (a grid), reward function, terminal states, transition model and a policy. The second part is to calculate the utility values of states using the value-iteration and policy-iteration algorithms. The third part is the implementation of four reinforcement learning algorithms and the last part is applications of these four algorithms to solve a simple sequential decision problem based on the defined MDP in the first part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1  Definition of a Markov decision process (MDP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Task: Import everything from the `mdp4e.py` module that contains definitions of classes of MDP and GridMDP\n",
    "'''\n",
    "\"Add your code here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `map` is a GridMDP object as shown below. The rewards are **+1** and **-1** in the terminal states, and **-0.05** in the rest. <img src=\"grid.png\"> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Task: Define reward R_s for all states except terminal states as -0.05\n",
    "'''\n",
    "R_s = \"Add your code here\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Task: Use R_s to define the maps according to the map given as a nested list, where the external list represents the rows and the inner list represents the columns. The grey block is marked as None.\n",
    "'''\n",
    "maps = \"Add your code here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Task: Define the locations of two terminals, (4,3) and (4,2), as a list of tuples (or pairs)\n",
    "'''\n",
    "terminals = \"Add your code here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Task: Define the following parameters, discount factor (gamma) as 0.9, transition model (probability for intended direction, left and right) to be 0.7, 0.2 and 0.1, respectively.\n",
    "'''\n",
    "gamma = \"Add your code here\"\n",
    "intended = \"Add your code here\"\n",
    "left = \"Add your code here\"\n",
    "right = \"Add your code here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Task: Use method GridMDP(Map, Terminals, gamma, intended, left, right) to create the decision environment.\n",
    "''' \n",
    "env = \"Add your code here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Display the environment: \\n states{} \\n terminals {} \\n actions {}\\n mdp {}\".format(env.states,env.terminals, env.actlist, env.grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Task: Define the actions that the agent can take in each state. We define the four actions as north = (0, 1), south = (0,-1), west = (-1, 0), east = (1, 0).\n",
    "'''\n",
    "north = \"Add your code here\"\n",
    "south = \"Add your code here\"\n",
    "west = \"Add your code here\"\n",
    "east = \"Add your code here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a policy, which is fixed during the learning process. A policy is shown in the graph below <img src=\"state.png\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Task: defined a policy as a dictionary with the key being the state and the value being the action.\n",
    "''' \n",
    "policy = {\"Add your code here\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 3): (1, 0), (1, 3): (1, 0), (2, 3): (1, 0), (3, 3): (1, 0), (4, 3): None, (0, 2): (0, 1), (2, 2): (0, 1), (3, 2): (1, 0), (4, 2): None, (0, 1): (0, 1), (1, 1): (0, 1), (2, 1): (0, 1), (3, 1): (0, 1), (4, 1): (-1, 0), (0, 0): (0, 1), (1, 0): (1, 0), (2, 0): (-1, 0), (3, 0): (-1, 0), (4, 0): (-1, 0)}\n"
     ]
    }
   ],
   "source": [
    "# Display the policy\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Calculate the utility values using value iteration and policy iteration algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following FIVE code segments (marked by code-segment x, where x = 1,2,...,5) calculate the utility values of each state using the value iteration and policy iteration methods, which are implemented in mdp4e.py. The results are used as the references for checking the results from a chosen RL agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code-swgment 1: Set the intial values of states\n",
    "U_init = {s: 0 for s in env.states}\n",
    "U_init[3,0]=-1\n",
    "U_init[4,0]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code-segment 2: Calculate utility values using the policy_iteration algorithm which returns a policy and the estimated utility values under that policy and display the policy.\n",
    "pi = policy_iteration(env)\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code-segment 3: Calculate the utility values of states using the policyiteration method\n",
    "U_values_policy_iteration=policy_evaluation(pi, U_init, env,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code-segment 4: Calculate utility values using the value_iteration algorithm.\n",
    "U_values_value_iteration = value_iteration(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State, estimated U value using value iteration and policy iteration:\n",
      "\n",
      "(0, 0),\t0.01386733860709485,\t0.013885974900681525\n",
      "(0, 1),\t0.08716574252667408,\t0.08717398026630825\n",
      "(0, 2),\t0.17165264383279072,\t0.17165583688280467\n",
      "(0, 3),\t0.2782665386931205,\t0.27826787448324986\n",
      "(1, 0),\t0.07183415412036656,\t0.0718543538976073\n",
      "(1, 1),\t0.14820517957160664,\t0.14821651758006482\n",
      "(1, 3),\t0.4170324574566717,\t0.41703274882033725\n",
      "(2, 0),\t0.1407947071323493,\t0.14080712521017613\n",
      "(2, 1),\t0.2620126493063095,\t0.2620169088331246\n",
      "(2, 2),\t0.416126500948199,\t0.4161267485291603\n",
      "(2, 3),\t0.5625933825620764,\t0.5625935026013431\n",
      "(3, 0),\t0.14224259739510386,\t0.1422520993748706\n",
      "(3, 1),\t0.2575315088745057,\t0.2575342677260213\n",
      "(3, 2),\t0.40877795818638724,\t0.40877807950072587\n",
      "(3, 3),\t0.7521829287103252,\t0.7521829599452016\n",
      "(4, 0),\t0.05176347055752232,\t0.05178031299606105\n",
      "(4, 1),\t0.0315587964786921,\t0.03156704500668436\n",
      "(4, 2),\t-1,\t-1.0\n",
      "(4, 3),\t1,\t1.0\n"
     ]
    }
   ],
   "source": [
    "# code-segment 5: Display the comparision of estimated utility values from both value-iteration and policy iteration algorithms \n",
    "temp = sorted(U_values_value_iteration.keys())\n",
    "print (\"State, estimated U value using value iteration and policy iteration:\\n\")\n",
    "for x in temp:\n",
    "        print(\"{},\\t{},\\t{}\".format(x,U_values_value_iteration[x],U_values_policy_iteration[x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 Implementation of four reinforcement learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following SIX code segments (marked by code-segment x, where x = 1,2,...,7) define the Four RL agents and two additional functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code-segment 1: Define the class of passive direct utility evaluation agent\n",
    "class PassiveDUEAgent:\n",
    "    def __init__(self, pi, mdp):\n",
    "        self.pi = pi\n",
    "        self.mdp = mdp\n",
    "        self.U = {}\n",
    "        self.s = None\n",
    "        self.a = None\n",
    "        self.s_history = []\n",
    "        self.r_history = []\n",
    "        self.init = mdp.init\n",
    "\n",
    "    def __call__(self, percept):\n",
    "        s1, r1 = percept\n",
    "        self.s_history.append(s1)\n",
    "        self.r_history.append(r1)\n",
    "        ##\n",
    "        ##\n",
    "        if s1 in self.mdp.terminals:\n",
    "            self.s = self.a = None\n",
    "        else:\n",
    "            self.s, self.a = s1, self.pi[s1]\n",
    "        return self.a\n",
    "\n",
    "    def estimate_U(self):\n",
    "        # this function can be called only if the MDP has reached a terminal state\n",
    "        # it will also reset the mdp history\n",
    "        assert self.a is None, 'MDP is not in terminal state'\n",
    "        assert len(self.s_history) == len(self.r_history)\n",
    "        # calculating the utilities based on the current iteration\n",
    "        U2 = {s: [] for s in set(self.s_history)}\n",
    "        for i in range(len(self.s_history)):\n",
    "            s = self.s_history[i]\n",
    "            U2[s] += [sum(self.r_history[i:])]\n",
    "        U2 = {k: sum(v) / max(len(v), 1) for k, v in U2.items()}\n",
    "        # resetting history\n",
    "        self.s_history, self.r_history = [], []\n",
    "        # setting the new utilities to the average of the previous\n",
    "        # iteration and this one\n",
    "        for k in U2.keys():\n",
    "            if k in self.U.keys():\n",
    "                self.U[k] = (self.U[k] + U2[k]) / 2\n",
    "            else:\n",
    "                self.U[k] = U2[k]\n",
    "        return self.U\n",
    "    \n",
    "    def update_state(self, percept):\n",
    "        \"\"\"To be overridden in most cases. The default case\n",
    "        assumes the percept to be of type (state, reward)\n",
    "        \"\"\"\n",
    "        return percept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code-segment 2: Define the passive TD agent class\n",
    "\n",
    "class PassiveTDAgent:\n",
    "    def __init__(self, pi, mdp, alpha=None):\n",
    "\n",
    "        self.pi = pi\n",
    "        self.U = {s: 0. for s in mdp.states}\n",
    "        self.Ns = {s: 0 for s in mdp.states}\n",
    "        self.s = None\n",
    "        self.a = None\n",
    "        self.r = None\n",
    "        self.gamma = mdp.gamma\n",
    "        self.terminals = mdp.terminals\n",
    "\n",
    "        if alpha:\n",
    "            self.alpha = alpha\n",
    "        else:\n",
    "            self.alpha = lambda n: 1 / (1 + n)  # udacity video\n",
    "\n",
    "    def __call__(self, percept):\n",
    "        s1, r1 = self.update_state(percept)\n",
    "        pi, U, Ns, s, r = self.pi, self.U, self.Ns, self.s, self.r\n",
    "        alpha, gamma, terminals = self.alpha, self.gamma, self.terminals\n",
    "        if not Ns[s1]:\n",
    "            U[s1] = r1\n",
    "        if s is not None:\n",
    "            Ns[s] += 1\n",
    "            U[s] += alpha(Ns[s]) * (r + gamma * U[s1] - U[s])\n",
    "        if s1 in terminals:\n",
    "            self.s = self.a = self.r = None\n",
    "        else:\n",
    "            self.s, self.a, self.r = s1, pi[s1], r1\n",
    "        #print (\"\\nNs {} \\n\\n s {} \\t a {} \\t r{} \\n\\n U {}\".format(Ns, self.s, self.a, self.r, U))\n",
    "        return self.a\n",
    "\n",
    "    def update_state(self, percept):\n",
    "        \"\"\"To be overridden in most cases. The default case\n",
    "        assumes the percept to be of type (state, reward).\"\"\"\n",
    "        return percept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code-segment 3: Define a class for passive ADP agent\n",
    "class PassiveADPAgent:\n",
    "    class ModelMDP(MDP):\n",
    "        \"\"\"Class for implementing modified Version of input MDP with\n",
    "        an editable transition model P and a custom function T.\"\"\"\n",
    "\n",
    "        def __init__(self, init, actlist, terminals, gamma, states):\n",
    "            super().__init__(init, actlist, terminals, states=states, gamma=gamma)\n",
    "            nested_dict = lambda: defaultdict(nested_dict)\n",
    "            # StackOverflow:whats-the-best-way-to-initialize-a-dict-of-dicts-in-python\n",
    "            self.P = nested_dict()\n",
    "\n",
    "        def T(self, s, a):\n",
    "            \"\"\"Return a list of tuples with probabilities for states\n",
    "            based on the learnt model P.\"\"\"\n",
    "            return [(prob, res) for (res, prob) in self.P[(s, a)].items()]\n",
    "\n",
    "    def __init__(self, pi, mdp):\n",
    "        self.pi = pi\n",
    "        self.mdp = PassiveADPAgent.ModelMDP(mdp.init, mdp.actlist,\n",
    "                                            mdp.terminals, mdp.gamma, mdp.states)\n",
    "        self.U = {}\n",
    "        self.Nsa = defaultdict(int)\n",
    "        self.Ns1_sa = defaultdict(int)\n",
    "        self.s = None\n",
    "        self.a = None\n",
    "        self.visited = set()  # keeping track of visited states\n",
    "\n",
    "    def __call__(self, percept):\n",
    "        s1, r1 = percept\n",
    "        mdp = self.mdp\n",
    "        R, P, terminals, pi = mdp.reward, mdp.P, mdp.terminals, self.pi\n",
    "        s, a, Nsa, Ns1_sa, U = self.s, self.a, self.Nsa, self.Ns1_sa, self.U\n",
    "        #print (\"\\n s {}, a {}, \\n\\n Nsa{}, \\n\\nNs1_sa{}, \\n\\nU{} \".format( s, a, Nsa, Ns1_sa, U)) \n",
    "\n",
    "        if s1 not in self.visited:  # Reward is only known for visited state.\n",
    "            U[s1] = R[s1] = r1\n",
    "            self.visited.add(s1)\n",
    "        if s is not None:\n",
    "            Nsa[(s, a)] += 1\n",
    "            Ns1_sa[(s1, s, a)] += 1\n",
    "            # for each t such that Nsâ€²|sa [t, s, a] is nonzero\n",
    "            for t in [res for (res, state, act), freq in Ns1_sa.items()\n",
    "                      if (state, act) == (s, a) and freq != 0]:\n",
    "                P[(s, a)][t] = Ns1_sa[(t, s, a)] / Nsa[(s, a)]\n",
    "                #print(\"\\nProbability of from {} to \\t{} is \\t{}\".format((s, a), t, P[(s, a)][t]))\n",
    "        self.U = policy_evaluation(pi, U, mdp)\n",
    "        ##\n",
    "        ##\n",
    "        self.Nsa, self.Ns1_sa = Nsa, Ns1_sa\n",
    "        if s1 in terminals:\n",
    "            self.s = self.a = None\n",
    "        else:\n",
    "            self.s, self.a = s1, self.pi[s1]\n",
    "        #print (\"\\n s {}, a {}, \\n\\n Nsa{}, \\n\\nNs1_sa{}, \\n\\nU{} \".format( self.s, self.a, Nsa, Ns1_sa, U)) \n",
    "        return self.a\n",
    "\n",
    "    def update_state(self, percept):\n",
    "        \"\"\"To be overridden in most cases. The default case\n",
    "        assumes the percept to be of type (state, reward).\"\"\"\n",
    "        return percept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code-segment 4: Define a class for an exploratory Q-learning agent\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, mdp, Ne, Rplus, alpha=None):\n",
    "\n",
    "        self.gamma = mdp.gamma\n",
    "        self.terminals = mdp.terminals\n",
    "        self.all_act = mdp.actlist\n",
    "        self.Ne = Ne  # iteration limit in exploration function\n",
    "        self.Rplus = Rplus  # large value to assign before iteration limit\n",
    "        self.Q = defaultdict(float)\n",
    "        self.Nsa = defaultdict(float)\n",
    "        self.s = None\n",
    "        self.a = None\n",
    "        self.r = None\n",
    "\n",
    "        if alpha:\n",
    "            self.alpha = alpha\n",
    "        else:\n",
    "            self.alpha = lambda n: 1. / (1 + n)  # udacity video\n",
    "\n",
    "    def f(self, u, n):\n",
    "        \"\"\"Exploration function. Returns fixed Rplus until\n",
    "        agent has visited state, action a Ne number of times.\n",
    "        Same as ADP agent in book.\"\"\"\n",
    "        if n < self.Ne:\n",
    "            return self.Rplus\n",
    "        else:\n",
    "            return u\n",
    "\n",
    "    def actions_in_state(self, state):\n",
    "        \"\"\"Return actions possible in given state.\n",
    "        Useful for max and argmax.\"\"\"\n",
    "        if state in self.terminals:\n",
    "            return [None]\n",
    "        else:\n",
    "            return self.all_act\n",
    "\n",
    "    def __call__(self, percept):\n",
    "        s1, r1 = self.update_state(percept)\n",
    "        Q, Nsa, s, a, r = self.Q, self.Nsa, self.s, self.a, self.r\n",
    "        alpha, gamma, terminals = self.alpha, self.gamma, self.terminals,\n",
    "        actions_in_state = self.actions_in_state\n",
    "\n",
    "        if s in terminals:\n",
    "            Q[s, None] = r1\n",
    "        if s is not None:\n",
    "            Nsa[s, a] += 1\n",
    "            Q[s, a] += alpha(Nsa[s, a]) * (r + gamma * max(Q[s1, a1]\n",
    "                                                           for a1 in actions_in_state(s1)) - Q[s, a])\n",
    "        if s in terminals:\n",
    "            self.s = self.a = self.r = None\n",
    "        else:\n",
    "            self.s, self.r = s1, r1\n",
    "            self.a = max(actions_in_state(s1), key=lambda a1: self.f(Q[s1, a1], Nsa[s1, a1]))\n",
    "        return self.a\n",
    "\n",
    "    def update_state(self, percept):\n",
    "        \"\"\"To be overridden in most cases. The default case\n",
    "        assumes the percept to be of type (state, reward).\"\"\"\n",
    "        return percept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code-segment 5: Define a function to trial one sequence\n",
    "def run_single_trial(agent_program, mdp):\n",
    "    \"\"\"Execute trial for given agent_program\n",
    "    and mdp. mdp should be an instance of subclass\n",
    "    of mdp.MDP \"\"\"\n",
    "\n",
    "    def take_single_action(mdp, s, a):\n",
    "        \"\"\"\n",
    "        Select outcome of taking action a\n",
    "        in state s. Weighted Sampling.\n",
    "        \"\"\"\n",
    "        x = random.uniform(0, 1)\n",
    "        cumulative_probability = 0.0\n",
    "        for probability_state in mdp.T(s, a):\n",
    "            probability, state = probability_state\n",
    "            cumulative_probability += probability\n",
    "            if x < cumulative_probability:\n",
    "                break\n",
    "        return state\n",
    "\n",
    "    current_state = mdp.init\n",
    "    sequence = []\n",
    "    while True:       \n",
    "        current_reward = mdp.R(current_state)\n",
    "        percept = (current_state, current_reward)\n",
    "        sequence.append(percept)\n",
    "        next_action = agent_program(percept)\n",
    "        if next_action is None:\n",
    "            #print (\"\\nSequence{}\".format(sequence))\n",
    "            sequence = []\n",
    "            break\n",
    "        current_state = take_single_action(mdp, current_state, next_action)\n",
    "        \n",
    "    if hasattr(agent_program, 'estimate_U'):\n",
    "        results = agent_program.estimate_U()\n",
    "        #print (\"\\n utility values {}\".format(results))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code-segment 6: Define a function to convert the Q Values above into U estimates.\n",
    "\n",
    "def convert_value_estimate(states):\n",
    "\tU = defaultdict(lambda: -1000.) # Very Large Negative Value for Comparison see below.\n",
    "\tfor state_action, value in states.items():\n",
    "\t    state, action = state_action\n",
    "\t    if U[state] < value:\n",
    "\t        U[state] = value\n",
    "\treturn U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 Apply the four RL algorithms to solve a sequential decision problem\n",
    "***First***, take an integer in the range of 1-4 from the user and save it to `model_option` to represent a model.\n",
    "\n",
    "model_option: 1-Passive DUEagent, 2-PassiveTDAgent, 3-PassiveADPagent, or 4-Q-LearningAgent\n",
    "\n",
    "If a user enters a number  > 4, print out a message \"invalid option number. Try again\".\n",
    "\n",
    "***Second***, Develop the model by running a numbre of trials to gain experieence\n",
    "\n",
    "***Lastly***, Compare the learned utility values from the chosen agent and the ones from the value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Task: Define an integer variable, say 'num_run' to define the number of trials to run to gain experience to estimate Utility values. You can try num_run = 200.\n",
    "''' \n",
    "\"Add your code here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a RL agent\n",
    "model_option = int(input(\"Choose a RL agent from the following: 1-Passive DUEagent, 2-PassiveTDAgent, 3-PassiveADPagent, or 4-Q-LearningAgent \\n your choice is: \"))\n",
    "\n",
    "if model_option == 1: # Passive DUEagent\n",
    "    \"\"\"\n",
    "    Task 1-1: Create an instance of PassiveDUEAgentclass by calling the constructor PassiveDUEAgent(policy, environment) using the environment created for RL in the environment notebook, env\n",
    "    \"\"\"\n",
    "    DUEagent = \"Add your code here\"\n",
    "    \n",
    "\n",
    "    \n",
    "    '''\n",
    "    Task 1-2: Run a number of trials (num_run) for the agent to estimate Utilities using a for-loop \n",
    "    '''\n",
    "    for i in range(\"Add your code here\"):\n",
    "        #print (\"\\nTrial {}\\n\".format(i))\n",
    "\n",
    "        # Invoke the method run_single_trial(agent-program, decision environment) with the passive DUE agent and environment created from the environment notebook, env\n",
    "        \"Add your code here\"\n",
    "    \n",
    "\n",
    "    \n",
    "    # Display the final utility values\n",
    "    print('\\n'.join([str(k)+':'+str(v) for k, v in DUEagent.U.items()]))\n",
    "\n",
    "    # Display the comparision results \n",
    "    b = sorted(DUEagent.U.keys())\n",
    "    \n",
    "    print (\"State, estimated U value using a DUEagent and estimated U value using value iteration are listed below:\\n\")\n",
    "    for x in b:\n",
    "        print(\"{},\\t{},\\t{},\\t{}\".format(x,DUEagent.U[x],U_values_value_iteration[x],U_values_policy_iteration[x]))\n",
    "    \n",
    "elif model_option == 2: # PassiveTDAgent\n",
    "    '''\n",
    "    Task 2-1: Create an instance of PassiveTDAgentclass by calling the constructor PassiveTDAgent(policy, environment,alpha) \n",
    "    using the environment created for RL in the environment notebook, env, and \n",
    "    alpha = lambda n: 60./(59+n)\n",
    "\n",
    "    '''\n",
    "    TDagent = \"Add your code here\"\n",
    "\n",
    "   \n",
    "    \"\"\"\n",
    "    Task 2-2: Run a number of trials (`num_run`) for the agent to estimate Utilities using a for-loop \n",
    "    \"\"\"\n",
    "    for i in range(\"Add your code here\"):\n",
    "        # Display the index of iteration in the for-loop\n",
    "        #print (\"\\nTrial {}\\n\".format(i))\n",
    "   \n",
    "        \"Add your code here\"\n",
    "\n",
    "    \n",
    "    # Display the final utility values\n",
    "    #print('\\n'.join([str(k)+':'+str(v) for k, v in TDagent.U.items()]))\n",
    "\n",
    "    # Display the comparision results \n",
    "    b = sorted(U_values_value_iteration.keys())\n",
    "    \n",
    "    print (\"State, estimated U value using a TDAgent and estimated U value using value iteration are listed below:\\n\")\n",
    "    for x in b:\n",
    "        print(\"{},\\t{},\\t{},\\t{}\".format(x,TDagent.U[x],U_values_value_iteration[x],U_values_policy_iteration[x]))\n",
    "    \n",
    "elif model_option == 3: # PassiveADPagent\n",
    "    '''\n",
    "    Task 3-1: Create an instance of PassiveADPAgent class by calling the constructor PassiveADPAgent(policy, environment) using the environment created for RL in the environment notebook, env\n",
    "    '''                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
    "    ADPagent = \"Add your code here\"\n",
    "\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Task 3-2: Run a number of trials (`num_run`) for the agent to estimate Utilities using a for-loop \n",
    "    '''\n",
    "    for i in range(\"Add your code here\"):\n",
    "        # Display the index of iteration in the for-loop\n",
    "        print (\"\\nTrial {}\\n\".format(i))\n",
    "   \n",
    "        # Invoke the method run_single_trial(agent-program, decision environment) with the passive ADP agent and environment created from the environment notebook, env\n",
    "        \"Add your code here\"\n",
    "\n",
    "    \n",
    "    # Display the final utility values\n",
    "    #print('\\n'.join([str(k)+':'+str(v) for k, v in ADPagent.U.items()]))\n",
    "\n",
    "    # Display the comparision results \n",
    "    b = sorted(U_values_value_iteration.keys())\n",
    "    \n",
    "    print (\"State, estimated U value using an ADP Agent and estimated U value using value iteration are listed below:\\n\")\n",
    "    for x in b:\n",
    "        print(\"{},\\t{},\\t{},\\t{}\".format(x,ADPagent.U[x],U_values_value_iteration[x],U_values_policy_iteration[x]))\n",
    "    \n",
    "elif model_option == 4: # Q-LearningAgent\n",
    "    '''\n",
    "    Task 4-1: Define required parameters to run an exploratory Q-learning agent, Rplus = 2 and Ne = 50\n",
    "    ''' \n",
    "    Rplus = \"Add your code here\"\n",
    "    Ne = \"Add your code here\"\n",
    "\n",
    "   \n",
    "    '''\n",
    "    Task 4-2: use the constructor QLearningAgent(environment, Ne, Rplus, alpha) to create an instance of Q-learning clasth the environment created in the environment notebook, env, parameters used to run an exploratory Q-learning agent, such as Rplus = 2 and Ne = 50 and alpha = lambda n: 60./(59+n) alpha = lambda n: 60./(59+n).\n",
    "    '''\n",
    "    q_agent = \"Add your code here\"\n",
    "\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    Task 4-3: Run a number of trials (`num_run`) for the agent to estimate Utilities using a for-loop \n",
    "    \"\"\"\n",
    "    for i in range(\"Add your code here\"):\n",
    "        # Display the index of iteration in the for-loop\n",
    "        print (\"\\nTrial {}\\n\".format(i))\n",
    "    \n",
    "        # Invoke the method `run_single_trial(instance of q_agent, decision environment)`, where `decision environment` is the one from the environment notebook, represented by `env`\n",
    "        \"Add your code here\"\n",
    "\n",
    "\n",
    "\n",
    "    # Display the Q values\n",
    "    #print ('\\n'.join([str(c1)+','+str(c2) for c1, c2 in q_agent.Q.items()]))\n",
    "       \n",
    "                    \n",
    "    \"\"\"\n",
    "    Task 4-4: Convert the Q Values above (q_agent.Q) to Utility values, stored in a variable `U` by calling `convert_value_estimate()` method with parameter: q_agent.Q  \n",
    "    \"\"\"\n",
    "    U = \"Add your code here\"\n",
    "\n",
    "        \n",
    "    # Display the utility values\n",
    "    #print('\\n'.join([str(k)+':'+str(v) for k, v in U.items()]))\n",
    "    \n",
    "    # Display the comparision results \n",
    "    b = sorted(U_values_value_iteration.keys())\n",
    "   \n",
    "    print (\"State, estimated U value using a q-Agent and estimated U value using value iteration are listed below:\\n\")\n",
    "    for x in b:\n",
    "        #print(\"{},\\t\\t\\t{},\\t\\t\\t{},\\t\\t\\t{}\".format(x,U[x],U_values_value_iteration[x],U_values_policy_iteration[x]))\n",
    "        print(f\"state: {x},U:{U[x]:9.6f},U_values_value_iteration:{U_values_value_iteration[x]:9.6f},U_values_policy_iteration:{U_values_policy_iteration[x]:9.6f}\")\n",
    "\n",
    "else:\n",
    "    print(\"invalid option number. Try again\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
